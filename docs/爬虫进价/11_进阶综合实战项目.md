# 第十一章：进阶综合实战项目

> 本章将综合运用整个进阶教程所学的所有技术，实现一个完整的社交媒体数据采集与分析工具。该项目将包含登录认证、反检测浏览器自动化、代理 IP 轮换、数据清洗存储、词云分析等完整功能链路。

## 11.1 项目概述

### 项目目标

构建一个类似 MediaCrawler 简化版的社交媒体数据采集工具，具备以下能力：

- **多种登录方式**：支持 Cookie 注入和扫码登录
- **反检测能力**：使用 Playwright + stealth.js 绕过检测
- **代理支持**：集成代理池实现 IP 轮换
- **多格式存储**：支持 JSON、CSV、数据库等多种存储方式
- **数据分析**：自动生成词云和统计报告

### 技术栈

| 模块 | 技术选型 |
|------|----------|
| 配置管理 | pydantic-settings |
| 日志系统 | loguru |
| 浏览器自动化 | Playwright |
| HTTP 客户端 | httpx |
| 数据验证 | Pydantic |
| 数据分析 | pandas + jieba + wordcloud |

### 项目结构

```
11_进阶综合实战项目/
├── config/              # 配置模块
│   ├── __init__.py
│   └── settings.py      # 配置定义
├── core/                # 核心模块
│   ├── __init__.py
│   └── browser.py       # 浏览器管理
├── login/               # 登录模块
│   ├── __init__.py
│   └── auth.py          # 认证实现
├── crawler/             # 爬虫模块
│   ├── __init__.py
│   └── spider.py        # 爬虫实现
├── store/               # 存储模块
│   ├── __init__.py
│   └── backend.py       # 存储后端
├── proxy/               # 代理模块
│   ├── __init__.py
│   └── pool.py          # 代理池
├── analysis/            # 分析模块
│   ├── __init__.py
│   └── report.py        # 报告生成
└── main.py              # 入口文件
```

## 11.2 配置模块设计

使用 pydantic-settings 实现类型安全的配置管理：

```python
# config/settings.py
from pydantic_settings import BaseSettings
from pydantic import Field
from typing import Optional, List
from enum import Enum


class StorageType(str, Enum):
    """存储类型"""
    JSON = "json"
    CSV = "csv"
    DB = "db"


class LoginType(str, Enum):
    """登录类型"""
    COOKIE = "cookie"
    QRCODE = "qrcode"


class Settings(BaseSettings):
    """项目配置"""

    # 基础配置
    app_name: str = "SocialCrawler"
    debug: bool = False

    # 浏览器配置
    browser_headless: bool = True
    browser_timeout: int = 30000
    browser_user_data_dir: Optional[str] = None

    # 登录配置
    login_type: LoginType = LoginType.COOKIE
    cookie_file: str = "cookies.json"

    # 代理配置
    proxy_enabled: bool = False
    proxy_api_url: Optional[str] = None

    # 爬虫配置
    crawl_max_pages: int = 10
    crawl_delay_min: float = 1.0
    crawl_delay_max: float = 3.0

    # 存储配置
    storage_type: StorageType = StorageType.JSON
    storage_output_dir: str = "./output"

    # 数据库配置（可选）
    db_host: str = "localhost"
    db_port: int = 3306
    db_user: str = "root"
    db_password: str = ""
    db_name: str = "crawler"

    class Config:
        env_file = ".env"
        env_prefix = "CRAWLER_"


# 全局配置实例
settings = Settings()
```

## 11.3 核心浏览器模块

封装 Playwright 浏览器管理，集成反检测功能：

```python
# core/browser.py
import asyncio
from typing import Optional
from playwright.async_api import async_playwright, Browser, BrowserContext, Page
from loguru import logger

# stealth.min.js 内容（简化版）
STEALTH_JS = """
// 隐藏 webdriver 属性
Object.defineProperty(navigator, 'webdriver', {
    get: () => undefined
});

// 修改 plugins
Object.defineProperty(navigator, 'plugins', {
    get: () => [1, 2, 3, 4, 5]
});

// 修改 languages
Object.defineProperty(navigator, 'languages', {
    get: () => ['zh-CN', 'zh', 'en']
});
"""


class BrowserManager:
    """浏览器管理器"""

    def __init__(
        self,
        headless: bool = True,
        timeout: int = 30000,
        user_data_dir: str = None,
        proxy: str = None
    ):
        self.headless = headless
        self.timeout = timeout
        self.user_data_dir = user_data_dir
        self.proxy = proxy

        self._playwright = None
        self._browser: Optional[Browser] = None
        self._context: Optional[BrowserContext] = None

    async def start(self) -> BrowserContext:
        """启动浏览器"""
        self._playwright = await async_playwright().start()

        # 浏览器启动参数
        launch_args = [
            '--disable-blink-features=AutomationControlled',
            '--no-sandbox',
        ]

        # 启动浏览器
        self._browser = await self._playwright.chromium.launch(
            headless=self.headless,
            args=launch_args
        )

        # 创建上下文
        context_options = {
            'viewport': {'width': 1920, 'height': 1080},
            'user_agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
        }

        if self.proxy:
            context_options['proxy'] = {'server': self.proxy}

        self._context = await self._browser.new_context(**context_options)

        # 注入反检测脚本
        await self._context.add_init_script(STEALTH_JS)

        logger.info("浏览器启动成功")
        return self._context

    async def new_page(self) -> Page:
        """创建新页面"""
        if not self._context:
            await self.start()
        page = await self._context.new_page()
        page.set_default_timeout(self.timeout)
        return page

    async def close(self):
        """关闭浏览器"""
        if self._context:
            await self._context.close()
        if self._browser:
            await self._browser.close()
        if self._playwright:
            await self._playwright.stop()
        logger.info("浏览器已关闭")

    async def __aenter__(self):
        await self.start()
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        await self.close()
```

## 11.4 登录认证模块

支持 Cookie 注入和扫码登录两种方式：

```python
# login/auth.py
import json
import asyncio
from abc import ABC, abstractmethod
from pathlib import Path
from typing import Optional, List, Dict
from playwright.async_api import BrowserContext, Page
from loguru import logger


class BaseAuth(ABC):
    """认证基类"""

    @abstractmethod
    async def login(self, context: BrowserContext) -> bool:
        """执行登录"""
        pass

    @abstractmethod
    async def check_login(self, page: Page) -> bool:
        """检查登录状态"""
        pass


class CookieAuth(BaseAuth):
    """Cookie 认证"""

    def __init__(self, cookie_file: str, domain: str):
        self.cookie_file = Path(cookie_file)
        self.domain = domain

    async def login(self, context: BrowserContext) -> bool:
        """通过 Cookie 登录"""
        if not self.cookie_file.exists():
            logger.error(f"Cookie 文件不存在: {self.cookie_file}")
            return False

        try:
            with open(self.cookie_file, 'r') as f:
                cookies = json.load(f)

            # 注入 Cookie
            await context.add_cookies(cookies)
            logger.info(f"成功注入 {len(cookies)} 个 Cookie")
            return True
        except Exception as e:
            logger.error(f"Cookie 注入失败: {e}")
            return False

    async def check_login(self, page: Page) -> bool:
        """检查登录状态"""
        # 检查是否存在登录标识元素
        try:
            # 这里需要根据实际网站调整选择器
            await page.wait_for_selector('.user-avatar', timeout=5000)
            return True
        except Exception:
            return False

    async def save_cookies(self, context: BrowserContext):
        """保存 Cookie"""
        cookies = await context.cookies()
        with open(self.cookie_file, 'w') as f:
            json.dump(cookies, f, indent=2)
        logger.info(f"Cookie 已保存到: {self.cookie_file}")


class QRCodeAuth(BaseAuth):
    """扫码认证"""

    def __init__(
        self,
        login_url: str,
        qrcode_selector: str,
        success_selector: str,
        save_cookie_file: str = None
    ):
        self.login_url = login_url
        self.qrcode_selector = qrcode_selector
        self.success_selector = success_selector
        self.save_cookie_file = save_cookie_file

    async def login(self, context: BrowserContext) -> bool:
        """扫码登录"""
        page = await context.new_page()

        try:
            # 访问登录页
            await page.goto(self.login_url)
            logger.info(f"访问登录页: {self.login_url}")

            # 等待二维码出现
            qrcode = await page.wait_for_selector(self.qrcode_selector, timeout=10000)

            # 截图保存二维码
            await qrcode.screenshot(path='qrcode.png')
            logger.info("二维码已保存到 qrcode.png，请扫码登录")

            # 等待登录成功
            await page.wait_for_selector(self.success_selector, timeout=120000)
            logger.info("扫码登录成功")

            # 保存 Cookie
            if self.save_cookie_file:
                cookies = await context.cookies()
                with open(self.save_cookie_file, 'w') as f:
                    json.dump(cookies, f, indent=2)
                logger.info(f"Cookie 已保存: {self.save_cookie_file}")

            return True
        except Exception as e:
            logger.error(f"扫码登录失败: {e}")
            return False
        finally:
            await page.close()

    async def check_login(self, page: Page) -> bool:
        """检查登录状态"""
        try:
            await page.wait_for_selector(self.success_selector, timeout=5000)
            return True
        except Exception:
            return False


class AuthManager:
    """认证管理器"""

    def __init__(self):
        self._auth: Optional[BaseAuth] = None

    def set_cookie_auth(self, cookie_file: str, domain: str):
        """设置 Cookie 认证"""
        self._auth = CookieAuth(cookie_file, domain)

    def set_qrcode_auth(
        self,
        login_url: str,
        qrcode_selector: str,
        success_selector: str,
        save_cookie_file: str = None
    ):
        """设置扫码认证"""
        self._auth = QRCodeAuth(
            login_url, qrcode_selector, success_selector, save_cookie_file
        )

    async def login(self, context: BrowserContext) -> bool:
        """执行登录"""
        if not self._auth:
            logger.warning("未配置认证方式")
            return False
        return await self._auth.login(context)

    async def check_login(self, page: Page) -> bool:
        """检查登录状态"""
        if not self._auth:
            return False
        return await self._auth.check_login(page)
```

## 11.5 数据存储模块

支持多种存储后端：

```python
# store/backend.py
import json
import csv
import os
from abc import ABC, abstractmethod
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any, Optional
from loguru import logger


class BaseStorage(ABC):
    """存储基类"""

    @abstractmethod
    async def save(self, data: List[Dict]) -> bool:
        """保存数据"""
        pass

    @abstractmethod
    async def load(self) -> List[Dict]:
        """加载数据"""
        pass


class JSONStorage(BaseStorage):
    """JSON 存储"""

    def __init__(self, output_dir: str, filename: str = None):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)

        if filename:
            self.filepath = self.output_dir / filename
        else:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            self.filepath = self.output_dir / f"data_{timestamp}.json"

    async def save(self, data: List[Dict]) -> bool:
        """保存数据到 JSON 文件"""
        try:
            with open(self.filepath, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            logger.info(f"数据已保存到: {self.filepath}")
            return True
        except Exception as e:
            logger.error(f"保存失败: {e}")
            return False

    async def load(self) -> List[Dict]:
        """从 JSON 文件加载数据"""
        if not self.filepath.exists():
            return []
        with open(self.filepath, 'r', encoding='utf-8') as f:
            return json.load(f)

    async def append(self, data: Dict) -> bool:
        """追加单条数据"""
        existing = await self.load()
        existing.append(data)
        return await self.save(existing)


class CSVStorage(BaseStorage):
    """CSV 存储"""

    def __init__(self, output_dir: str, filename: str = None, fields: List[str] = None):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)

        if filename:
            self.filepath = self.output_dir / filename
        else:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            self.filepath = self.output_dir / f"data_{timestamp}.csv"

        self.fields = fields

    async def save(self, data: List[Dict]) -> bool:
        """保存数据到 CSV 文件"""
        if not data:
            return True

        try:
            fields = self.fields or list(data[0].keys())

            with open(self.filepath, 'w', encoding='utf-8-sig', newline='') as f:
                writer = csv.DictWriter(f, fieldnames=fields)
                writer.writeheader()
                writer.writerows(data)

            logger.info(f"数据已保存到: {self.filepath}")
            return True
        except Exception as e:
            logger.error(f"保存失败: {e}")
            return False

    async def load(self) -> List[Dict]:
        """从 CSV 文件加载数据"""
        if not self.filepath.exists():
            return []
        with open(self.filepath, 'r', encoding='utf-8-sig') as f:
            reader = csv.DictReader(f)
            return list(reader)


class StorageManager:
    """存储管理器"""

    def __init__(self, storage_type: str, output_dir: str, **kwargs):
        self.output_dir = output_dir

        if storage_type == 'json':
            self._storage = JSONStorage(output_dir, **kwargs)
        elif storage_type == 'csv':
            self._storage = CSVStorage(output_dir, **kwargs)
        else:
            raise ValueError(f"不支持的存储类型: {storage_type}")

    async def save(self, data: List[Dict]) -> bool:
        """保存数据"""
        return await self._storage.save(data)

    async def load(self) -> List[Dict]:
        """加载数据"""
        return await self._storage.load()
```

## 11.6 代理池模块

简化的代理池实现：

```python
# proxy/pool.py
import random
import asyncio
from typing import Optional, List
from dataclasses import dataclass, field
from datetime import datetime
import httpx
from loguru import logger


@dataclass
class ProxyInfo:
    """代理信息"""
    ip: str
    port: int
    protocol: str = "http"
    username: str = None
    password: str = None
    expire_time: datetime = None
    fail_count: int = 0
    success_count: int = 0

    @property
    def url(self) -> str:
        """获取代理 URL"""
        if self.username and self.password:
            return f"{self.protocol}://{self.username}:{self.password}@{self.ip}:{self.port}"
        return f"{self.protocol}://{self.ip}:{self.port}"

    @property
    def is_valid(self) -> bool:
        """检查是否有效"""
        if self.expire_time and datetime.now() > self.expire_time:
            return False
        return self.fail_count < 3


class ProxyPool:
    """代理池"""

    def __init__(self, api_url: str = None):
        self.api_url = api_url
        self._proxies: List[ProxyInfo] = []
        self._lock = asyncio.Lock()

    async def fetch_proxies(self, count: int = 10) -> List[ProxyInfo]:
        """从 API 获取代理"""
        if not self.api_url:
            logger.warning("未配置代理 API")
            return []

        try:
            async with httpx.AsyncClient() as client:
                response = await client.get(
                    self.api_url,
                    params={'count': count},
                    timeout=10
                )
                data = response.json()

                proxies = []
                for item in data.get('data', []):
                    proxy = ProxyInfo(
                        ip=item['ip'],
                        port=item['port'],
                        protocol=item.get('protocol', 'http'),
                        expire_time=datetime.fromisoformat(item['expire_time'])
                        if 'expire_time' in item else None
                    )
                    proxies.append(proxy)

                logger.info(f"获取 {len(proxies)} 个代理")
                return proxies
        except Exception as e:
            logger.error(f"获取代理失败: {e}")
            return []

    async def add_proxies(self, proxies: List[ProxyInfo]):
        """添加代理到池中"""
        async with self._lock:
            self._proxies.extend(proxies)

    async def get_proxy(self) -> Optional[ProxyInfo]:
        """获取一个可用代理"""
        async with self._lock:
            # 过滤有效代理
            valid_proxies = [p for p in self._proxies if p.is_valid]

            if not valid_proxies:
                # 尝试获取新代理
                new_proxies = await self.fetch_proxies()
                if new_proxies:
                    self._proxies = new_proxies
                    valid_proxies = new_proxies

            if not valid_proxies:
                return None

            # 随机选择一个
            return random.choice(valid_proxies)

    async def report_success(self, proxy: ProxyInfo):
        """报告代理成功"""
        proxy.success_count += 1
        proxy.fail_count = 0

    async def report_failure(self, proxy: ProxyInfo):
        """报告代理失败"""
        proxy.fail_count += 1

    async def remove_invalid(self):
        """移除无效代理"""
        async with self._lock:
            self._proxies = [p for p in self._proxies if p.is_valid]

    @property
    def size(self) -> int:
        """代理池大小"""
        return len(self._proxies)
```

## 11.7 爬虫模块

核心爬虫实现：

```python
# crawler/spider.py
import asyncio
import random
from typing import List, Dict, Any, Optional, Callable
from datetime import datetime
from playwright.async_api import Page, BrowserContext
from loguru import logger


class BaseCrawler:
    """爬虫基类"""

    def __init__(
        self,
        delay_min: float = 1.0,
        delay_max: float = 3.0,
        max_pages: int = 10
    ):
        self.delay_min = delay_min
        self.delay_max = delay_max
        self.max_pages = max_pages
        self._results: List[Dict] = []

    async def random_delay(self):
        """随机延迟"""
        delay = random.uniform(self.delay_min, self.delay_max)
        await asyncio.sleep(delay)

    async def crawl(self, page: Page) -> List[Dict]:
        """执行爬取（子类实现）"""
        raise NotImplementedError

    def get_results(self) -> List[Dict]:
        """获取结果"""
        return self._results


class ContentCrawler(BaseCrawler):
    """内容爬虫"""

    def __init__(
        self,
        start_url: str,
        item_selector: str,
        fields: Dict[str, str],
        next_page_selector: str = None,
        **kwargs
    ):
        """
        Args:
            start_url: 起始 URL
            item_selector: 列表项选择器
            fields: 字段映射 {字段名: 选择器}
            next_page_selector: 下一页按钮选择器
        """
        super().__init__(**kwargs)
        self.start_url = start_url
        self.item_selector = item_selector
        self.fields = fields
        self.next_page_selector = next_page_selector

    async def extract_item(self, element) -> Dict:
        """提取单个项目的数据"""
        data = {'crawl_time': datetime.now().isoformat()}

        for field_name, selector in self.fields.items():
            try:
                if selector.startswith('@'):
                    # 属性选择
                    attr = selector[1:]
                    value = await element.get_attribute(attr)
                else:
                    # 文本选择
                    sub_element = await element.query_selector(selector)
                    if sub_element:
                        value = await sub_element.inner_text()
                    else:
                        value = None

                data[field_name] = value.strip() if value else None
            except Exception as e:
                logger.debug(f"提取字段 {field_name} 失败: {e}")
                data[field_name] = None

        return data

    async def crawl(self, page: Page) -> List[Dict]:
        """执行爬取"""
        self._results = []
        current_page = 0

        # 访问起始页
        await page.goto(self.start_url)
        logger.info(f"访问: {self.start_url}")

        while current_page < self.max_pages:
            current_page += 1
            logger.info(f"正在爬取第 {current_page} 页")

            # 等待列表加载
            await page.wait_for_selector(self.item_selector, timeout=10000)

            # 提取数据
            items = await page.query_selector_all(self.item_selector)
            for item in items:
                data = await self.extract_item(item)
                self._results.append(data)

            logger.info(f"第 {current_page} 页提取 {len(items)} 条数据")

            # 翻页
            if self.next_page_selector and current_page < self.max_pages:
                try:
                    next_btn = await page.query_selector(self.next_page_selector)
                    if next_btn:
                        await next_btn.click()
                        await self.random_delay()
                    else:
                        logger.info("没有更多页面")
                        break
                except Exception as e:
                    logger.warning(f"翻页失败: {e}")
                    break

        logger.info(f"爬取完成，共 {len(self._results)} 条数据")
        return self._results


class ScrollCrawler(BaseCrawler):
    """滚动加载爬虫"""

    def __init__(
        self,
        start_url: str,
        item_selector: str,
        fields: Dict[str, str],
        scroll_count: int = 10,
        **kwargs
    ):
        super().__init__(**kwargs)
        self.start_url = start_url
        self.item_selector = item_selector
        self.fields = fields
        self.scroll_count = scroll_count

    async def crawl(self, page: Page) -> List[Dict]:
        """执行爬取"""
        self._results = []

        await page.goto(self.start_url)
        logger.info(f"访问: {self.start_url}")

        seen_ids = set()

        for i in range(self.scroll_count):
            logger.info(f"滚动加载第 {i + 1} 次")

            # 提取当前页面数据
            items = await page.query_selector_all(self.item_selector)
            for item in items:
                # 简单去重（基于内容哈希）
                content = await item.inner_text()
                content_id = hash(content)

                if content_id not in seen_ids:
                    seen_ids.add(content_id)
                    data = {'content': content.strip(), 'crawl_time': datetime.now().isoformat()}
                    self._results.append(data)

            # 滚动到底部
            await page.evaluate('window.scrollTo(0, document.body.scrollHeight)')
            await self.random_delay()

        logger.info(f"爬取完成，共 {len(self._results)} 条数据")
        return self._results
```

## 11.8 分析报告模块

数据分析和报告生成：

```python
# analysis/report.py
import os
from typing import List, Dict
from datetime import datetime
from collections import Counter
from loguru import logger

# 可选依赖
try:
    import jieba
    HAS_JIEBA = True
except ImportError:
    HAS_JIEBA = False

try:
    from wordcloud import WordCloud
    HAS_WORDCLOUD = True
except ImportError:
    HAS_WORDCLOUD = False


class DataAnalyzer:
    """数据分析器"""

    STOPWORDS = {
        '的', '是', '在', '了', '和', '与', '或', '有', '个', '人',
        '这', '那', '就', '都', '也', '为', '对', '到', '从', '把',
    }

    def __init__(self, data: List[Dict], output_dir: str = "./output"):
        self.data = data
        self.output_dir = output_dir
        os.makedirs(output_dir, exist_ok=True)

    def basic_stats(self) -> Dict:
        """基础统计"""
        return {
            'total_records': len(self.data),
            'fields': list(self.data[0].keys()) if self.data else [],
        }

    def word_frequency(self, text_field: str, top_n: int = 50) -> List[tuple]:
        """词频统计"""
        if not HAS_JIEBA:
            logger.warning("jieba 未安装，跳过词频分析")
            return []

        all_words = []
        for item in self.data:
            text = item.get(text_field, '')
            if text:
                words = jieba.lcut(str(text))
                words = [w for w in words if w not in self.STOPWORDS and len(w) > 1]
                all_words.extend(words)

        return Counter(all_words).most_common(top_n)

    def generate_wordcloud(self, text_field: str, output_file: str = "wordcloud.png", font_path: str = None) -> str:
        """生成词云"""
        if not HAS_WORDCLOUD:
            logger.warning("wordcloud 未安装，跳过词云生成")
            return ""

        word_freq = self.word_frequency(text_field)
        if not word_freq:
            return ""

        freq_dict = dict(word_freq)

        wc = WordCloud(
            width=1200,
            height=800,
            background_color='white',
            font_path=font_path,
            max_words=200,
        )
        wc.generate_from_frequencies(freq_dict)

        output_path = os.path.join(self.output_dir, output_file)
        wc.to_file(output_path)
        logger.info(f"词云已保存: {output_path}")
        return output_path


class ReportGenerator:
    """报告生成器"""

    def __init__(self, data: List[Dict], output_dir: str = "./output"):
        self.data = data
        self.output_dir = output_dir
        self.analyzer = DataAnalyzer(data, output_dir)
        os.makedirs(output_dir, exist_ok=True)

    def generate(self, text_field: str = None, font_path: str = None) -> str:
        """生成完整报告"""
        lines = []

        # 标题
        lines.append("# 数据采集分析报告")
        lines.append("")
        lines.append(f"> 生成时间: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        lines.append("")
        lines.append("---")
        lines.append("")

        # 基础统计
        stats = self.analyzer.basic_stats()
        lines.append("## 基础统计")
        lines.append("")
        lines.append(f"- 数据总量: {stats['total_records']} 条")
        lines.append(f"- 字段列表: {', '.join(stats['fields'])}")
        lines.append("")

        # 词频分析
        if text_field and HAS_JIEBA:
            lines.append("## 热门词汇 TOP 20")
            lines.append("")
            word_freq = self.analyzer.word_frequency(text_field, 20)
            lines.append("| 排名 | 词汇 | 频次 |")
            lines.append("| --- | --- | --- |")
            for i, (word, count) in enumerate(word_freq, 1):
                lines.append(f"| {i} | {word} | {count} |")
            lines.append("")

            # 词云
            if HAS_WORDCLOUD:
                wordcloud_path = self.analyzer.generate_wordcloud(text_field, font_path=font_path)
                if wordcloud_path:
                    lines.append("## 词云")
                    lines.append("")
                    lines.append(f"![词云](wordcloud.png)")
                    lines.append("")

        # 保存报告
        report_path = os.path.join(self.output_dir, "report.md")
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write('\n'.join(lines))

        logger.info(f"报告已保存: {report_path}")
        return report_path
```

## 11.9 主程序入口

整合所有模块：

```python
# main.py
import asyncio
from loguru import logger

# 导入各模块
from config.settings import settings
from core.browser import BrowserManager
from login.auth import AuthManager
from crawler.spider import ContentCrawler
from store.backend import StorageManager
from analysis.report import ReportGenerator


async def main():
    """主程序"""
    logger.info(f"启动 {settings.app_name}")

    # 初始化浏览器
    browser = BrowserManager(
        headless=settings.browser_headless,
        timeout=settings.browser_timeout
    )

    try:
        async with browser:
            context = await browser.start()

            # 登录认证
            auth = AuthManager()
            if settings.login_type == 'cookie':
                auth.set_cookie_auth(settings.cookie_file, domain="example.com")
            else:
                auth.set_qrcode_auth(
                    login_url="https://example.com/login",
                    qrcode_selector=".qrcode-img",
                    success_selector=".user-avatar",
                    save_cookie_file=settings.cookie_file
                )

            login_success = await auth.login(context)
            if not login_success:
                logger.error("登录失败")
                return

            # 创建爬虫
            page = await browser.new_page()
            crawler = ContentCrawler(
                start_url="https://example.com/list",
                item_selector=".item",
                fields={
                    "title": ".title",
                    "content": ".content",
                    "author": ".author",
                    "time": ".time",
                },
                next_page_selector=".next-page",
                max_pages=settings.crawl_max_pages,
                delay_min=settings.crawl_delay_min,
                delay_max=settings.crawl_delay_max
            )

            # 执行爬取
            results = await crawler.crawl(page)
            logger.info(f"爬取完成: {len(results)} 条数据")

            # 保存数据
            storage = StorageManager(
                storage_type=settings.storage_type.value,
                output_dir=settings.storage_output_dir
            )
            await storage.save(results)

            # 生成报告
            report = ReportGenerator(results, settings.storage_output_dir)
            report.generate(text_field='content')

            logger.info("任务完成")

    except Exception as e:
        logger.exception(f"执行出错: {e}")


if __name__ == "__main__":
    asyncio.run(main())
```

## 11.10 项目部署

### 依赖管理

创建 `requirements.txt`：

```
# 核心依赖
playwright>=1.40.0
httpx>=0.25.0
pydantic>=2.0.0
pydantic-settings>=2.0.0
loguru>=0.7.0

# 数据分析（可选）
pandas>=2.0.0
jieba>=0.42.0
wordcloud>=1.9.0

# 开发工具（可选）
pytest>=7.0.0
pytest-asyncio>=0.21.0
```

### Docker 部署

创建 `Dockerfile`：

```dockerfile
FROM python:3.11-slim

WORKDIR /app

# 安装系统依赖
RUN apt-get update && apt-get install -y \
    wget \
    && rm -rf /var/lib/apt/lists/*

# 复制项目文件
COPY requirements.txt .
RUN pip install -r requirements.txt

# 安装 Playwright 浏览器
RUN playwright install chromium
RUN playwright install-deps

COPY . .

CMD ["python", "main.py"]
```

### 定时任务

使用 cron 配置定时执行：

```bash
# 每天凌晨 2 点执行
0 2 * * * cd /path/to/project && python main.py >> /var/log/crawler.log 2>&1
```

## 本章小结

本章我们完成了一个完整的社交媒体数据采集与分析项目，综合运用了：

1. **工程化设计**
   - 模块化架构
   - 配置管理
   - 日志系统

2. **登录认证**
   - Cookie 注入
   - 扫码登录

3. **反检测技术**
   - stealth.js 注入
   - 浏览器指纹伪装

4. **数据处理**
   - 多格式存储
   - 数据分析
   - 词云生成

5. **部署运维**
   - Docker 容器化
   - 定时任务

**关键要点：**

- 项目架构要清晰，模块职责要单一
- 配置和代码分离，便于不同环境部署
- 登录状态要持久化，避免频繁登录
- 做好异常处理和日志记录
- 遵守网站规则，合理控制爬取频率

---

## 教程总结

恭喜你完成了 Python 爬虫进阶教程的全部内容！在这 11 章的学习中，你掌握了：

1. **工程化开发**：日志、配置、异常处理
2. **反爬对抗**：请求伪装、代理 IP
3. **浏览器自动化**：Playwright 基础和进阶
4. **登录认证**：Cookie 管理、扫码登录
5. **验证码处理**：OCR 识别、滑块验证
6. **数据处理**：清洗、去重、分析、可视化

这些技术都源自 MediaCrawler 等实际项目的生产实践，希望能帮助你在爬虫开发领域更进一步。

**最后的建议：**

- 持续关注反爬技术的演进
- 遵守法律法规和网站规则
- 参与开源项目，与社区共同成长
- 将爬虫作为数据获取的手段，关注数据本身的价值

祝你在数据采集的道路上越走越远！
