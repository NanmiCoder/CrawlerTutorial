# ç¬¬åä¸€ç« ï¼šè¿›é˜¶ç»¼åˆå®æˆ˜é¡¹ç›®

> æœ¬ç« å°†ç»¼åˆè¿ç”¨æ•´ä¸ªè¿›é˜¶æ•™ç¨‹æ‰€å­¦çš„æ‰€æœ‰æŠ€æœ¯ï¼Œä»¥ **Bç«™ï¼ˆbilibili.comï¼‰** ä¸ºç›®æ ‡ï¼Œå®ç°ä¸€ä¸ªå®Œæ•´çš„è§†é¢‘æ•°æ®é‡‡é›†ä¸åˆ†æå·¥å…·ã€‚è¯¥é¡¹ç›®å°†åŒ…å«ç™»å½•è®¤è¯ã€API ç­¾åã€æµè§ˆå™¨è‡ªåŠ¨åŒ–ã€æ•°æ®å­˜å‚¨ã€åˆ†ææŠ¥å‘Šç­‰å®Œæ•´åŠŸèƒ½é“¾è·¯ã€‚

## 11.1 é¡¹ç›®æ¦‚è¿°

### é¡¹ç›®ç›®æ ‡

æ„å»ºä¸€ä¸ªç±»ä¼¼ [MediaCrawler](https://github.com/NanmiCoder/MediaCrawler) ç®€åŒ–ç‰ˆçš„è§†é¢‘æ•°æ®é‡‡é›†å·¥å…·ï¼Œä»¥ **Bç«™** ä¸ºç›®æ ‡å¹³å°ï¼Œå…·å¤‡ä»¥ä¸‹èƒ½åŠ›ï¼š

- **ç™»å½•è®¤è¯**ï¼šæ”¯æŒæ‰«ç ç™»å½•å’Œ Cookie ç™»å½•
- **API ç­¾å**ï¼šå®ç° Bç«™ WBI ç­¾åç®—æ³•
- **è§†é¢‘æœç´¢**ï¼šæŒ‰å…³é”®è¯æœç´¢è§†é¢‘
- **è§†é¢‘è¯¦æƒ…**ï¼šè·å–å®Œæ•´è§†é¢‘ä¿¡æ¯ï¼ˆæ’­æ”¾é‡ã€ç‚¹èµã€æ”¶è—ç­‰ï¼‰
- **æ•°æ®å­˜å‚¨**ï¼šæ”¯æŒ JSONã€CSV ä¸¤ç§å­˜å‚¨æ–¹å¼
- **æ•°æ®åˆ†æ**ï¼šè‡ªåŠ¨ç”Ÿæˆè¯äº‘å’Œç»Ÿè®¡æŠ¥å‘Š

> **ç›®æ ‡ç½‘ç«™è¯´æ˜ï¼š**
> - ç½‘ç«™ï¼šhttps://www.bilibili.com
> - ç±»å‹ï¼šå›½å†…æœ€å¤§çš„è§†é¢‘ç¤¾åŒºå¹³å°
> - ç‰¹ç‚¹ï¼šéœ€è¦ç™»å½•è·å–å®Œæ•´æ•°æ®ï¼ŒAPI æœ‰ WBI ç­¾åä¿æŠ¤
> - æ•°æ®ï¼šè§†é¢‘æ ‡é¢˜ã€UPä¸»ã€æ’­æ”¾é‡ã€ç‚¹èµã€æ”¶è—ã€å¼¹å¹•æ•°ç­‰

### æ•´ä½“æ¶æ„å›¾

åœ¨å¼€å§‹ç¼–ç ä¹‹å‰ï¼Œè®©æˆ‘ä»¬å…ˆä»å®è§‚è§’åº¦ç†è§£æ•´ä¸ªé¡¹ç›®çš„æ¶æ„ï¼š

```mermaid
graph TB
    subgraph å…¥å£å±‚
        main["main.py<br/>ç¨‹åºå…¥å£"]
    end

    subgraph æ ¸å¿ƒæ¨¡å—
        config["config é…ç½®<br/>settings.py<br/>bilibili_config.py"]
        crawler["crawler çˆ¬è™«<br/>spider.py<br/>æ ¸å¿ƒè°ƒåº¦"]
        store["store å­˜å‚¨<br/>backend.py<br/>JSON/CSV"]
    end

    subgraph åŠŸèƒ½æ¨¡å—
        login["login ç™»å½•<br/>auth.py<br/>æ‰«ç /Cookie"]
        client["client å®¢æˆ·ç«¯<br/>bilibili_client.py<br/>APIè¯·æ±‚"]
        analysis["analysis åˆ†æ<br/>report.py<br/>è¯äº‘/ç»Ÿè®¡"]
    end

    subgraph åŸºç¡€æ¨¡å—
        core["core æµè§ˆå™¨<br/>browser.py<br/>Playwrightå°è£…"]
        tools["tools å·¥å…·<br/>sign.py<br/>WBIç­¾å"]
        models["models æ¨¡å‹<br/>bilibili.py<br/>æ•°æ®ç»“æ„"]
    end

    main --> config
    main --> crawler
    main --> store
    main --> analysis

    crawler --> login
    crawler --> client

    login --> core
    client --> tools
    tools --> models

    config -.->|é…ç½®æ³¨å…¥| crawler
    crawler -->|æ•°æ®| store
```

### çˆ¬è™«æ‰§è¡Œæµç¨‹

æ•´ä¸ªçˆ¬è™«ä»å¯åŠ¨åˆ°å®Œæˆçš„å®Œæ•´æµç¨‹å¦‚ä¸‹ï¼š

```mermaid
flowchart TD
    start([ç¨‹åºå¯åŠ¨ main.py]) --> step1

    subgraph step1 [æ­¥éª¤1: åˆå§‹åŒ–æµè§ˆå™¨]
        browser["BrowserManager.start()<br/>å¯åŠ¨ Playwright<br/>åˆ›å»º BrowserContext"]
    end

    step1 --> step2

    subgraph step2 [æ­¥éª¤2: ç™»å½•è®¤è¯]
        check{æ£€æŸ¥ç™»å½•çŠ¶æ€}
        check -->|å·²ç™»å½•| skip[è·³è¿‡ç™»å½•]
        check -->|æœªç™»å½•| login_flow
        subgraph login_flow [æ‰§è¡Œç™»å½•]
            qrcode["æ‰«ç ç™»å½•: æ˜¾ç¤ºäºŒç»´ç -ç­‰å¾…æ‰«ç -è·å–Cookie"]
            cookie["Cookieç™»å½•: æ³¨å…¥Cookie-éªŒè¯æœ‰æ•ˆæ€§"]
        end
    end

    step2 --> step3

    subgraph step3 [æ­¥éª¤3: åˆå§‹åŒ–APIå®¢æˆ·ç«¯]
        init["åŒæ­¥Cookieåˆ°httpx<br/>è·å–WBIç­¾åå¯†é’¥<br/>åˆå§‹åŒ–ç­¾åå™¨"]
    end

    step3 --> step4

    subgraph step4 [æ­¥éª¤4: æ‰§è¡Œçˆ¬å–ä»»åŠ¡]
        mode{çˆ¬å–æ¨¡å¼}
        mode -->|SEARCH| search["å…³é”®è¯æœç´¢-ç¿»é¡µè·å–-è·å–è¯¦æƒ…"]
        mode -->|DETAIL| detail[ç›´æ¥è·å–æŒ‡å®šè§†é¢‘è¯¦æƒ…]
        search --> request
        detail --> request
        request["æ¯æ¬¡è¯·æ±‚:<br/>æ„é€ å‚æ•°-WBIç­¾å-å‘é€è¯·æ±‚-è§£æå“åº”-éšæœºå»¶è¿Ÿ"]
    end

    step4 --> step5

    subgraph step5 [æ­¥éª¤5: æ•°æ®å­˜å‚¨]
        save["BilibiliVideoå¯¹è±¡åˆ—è¡¨<br/>è½¬æ¢ä¸ºå­—å…¸<br/>ä¿å­˜ä¸º JSON/CSV"]
    end

    step5 --> step6

    subgraph step6 [æ­¥éª¤6: ç”Ÿæˆåˆ†ææŠ¥å‘Š]
        report["ç»Ÿè®¡åˆ†æ-è¯é¢‘ç»Ÿè®¡<br/>ç”Ÿæˆè¯äº‘-è¾“å‡ºMarkdownæŠ¥å‘Š"]
    end

    step6 --> step7

    subgraph step7 [æ­¥éª¤7: æ¸…ç†èµ„æº]
        cleanup["å…³é—­æµè§ˆå™¨<br/>ä¿å­˜ç™»å½•çŠ¶æ€<br/>ç¨‹åºé€€å‡º"]
    end

    step7 --> finish([å®Œæˆ])
```

### æ•°æ®æµå‘å›¾

ç†è§£æ•°æ®åœ¨å„æ¨¡å—ä¹‹é—´å¦‚ä½•æµè½¬ï¼š

```mermaid
flowchart LR
    subgraph input [ç”¨æˆ·è¾“å…¥]
        keyword["å…³é”®è¯<br/>é…ç½®æ–‡ä»¶"]
    end

    subgraph process [ç³»ç»Ÿå¤„ç†]
        search["æœç´¢API<br/>(WBIç­¾å)"]
        detail["è¯¦æƒ…API<br/>(è·å–è¯¦æƒ…)"]
        validate["Pydanticæ•°æ®éªŒè¯<br/>BilibiliVideo"]
    end

    subgraph output [æœ€ç»ˆè¾“å‡º]
        json["JSONæ–‡ä»¶<br/>(ç»“æ„åŒ–)"]
        csv["CSVæ–‡ä»¶<br/>(è¡¨æ ¼åŒ–)"]
        report["åˆ†ææŠ¥å‘Š<br/>(Markdown)"]
        wordcloud["è¯äº‘å›¾ç‰‡<br/>(PNG)"]
    end

    keyword --> search
    search -->|è§†é¢‘åˆ—è¡¨| detail
    detail --> validate
    validate --> json
    validate --> csv
    validate --> report
    report --> wordcloud
```

### å‚è€ƒé¡¹ç›®

æœ¬é¡¹ç›®å‚è€ƒ [MediaCrawler](https://github.com/NanmiCoder/MediaCrawler) çš„ Bç«™å®ç°ï¼š

| æ–‡ä»¶ | è¯´æ˜ |
|------|------|
| `media_platform/bilibili/core.py` | çˆ¬è™«æ ¸å¿ƒé€»è¾‘ |
| `media_platform/bilibili/client.py` | API å®¢æˆ·ç«¯ |
| `media_platform/bilibili/login.py` | ç™»å½•è®¤è¯ |
| `media_platform/bilibili/help.py` | WBI ç­¾åç®—æ³• |

### æŠ€æœ¯æ ˆ

| æ¨¡å— | æŠ€æœ¯é€‰å‹ | ä½œç”¨ |
|------|----------|------|
| é…ç½®ç®¡ç† | pydantic-settings | ç±»å‹å®‰å…¨çš„é…ç½®ï¼Œæ”¯æŒç¯å¢ƒå˜é‡ |
| æ—¥å¿—ç³»ç»Ÿ | loguru | ä¼˜é›…çš„æ—¥å¿—è®°å½•å’Œè½®è½¬ |
| æµè§ˆå™¨è‡ªåŠ¨åŒ– | Playwright | å¤„ç†ç™»å½•ã€è·å–Cookieå’Œç­¾åå¯†é’¥ |
| HTTP å®¢æˆ·ç«¯ | httpx | å¼‚æ­¥HTTPè¯·æ±‚ï¼Œé«˜æ€§èƒ½ |
| æ•°æ®éªŒè¯ | Pydantic | æ•°æ®æ¨¡å‹å®šä¹‰å’ŒéªŒè¯ |
| æ•°æ®åˆ†æ | pandas + jieba + wordcloud | ç»Ÿè®¡åˆ†æå’Œå¯è§†åŒ– |

### é¡¹ç›®ç»“æ„

```
11_è¿›é˜¶ç»¼åˆå®æˆ˜é¡¹ç›®/
â”œâ”€â”€ config/              # é…ç½®æ¨¡å—
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ settings.py      # é€šç”¨é…ç½®
â”‚   â””â”€â”€ bilibili_config.py  # Bç«™ç‰¹å®šé…ç½®
â”œâ”€â”€ core/                # æ ¸å¿ƒæ¨¡å—
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ browser.py       # æµè§ˆå™¨ç®¡ç†
â”œâ”€â”€ login/               # ç™»å½•æ¨¡å—
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ auth.py          # Bç«™ç™»å½•è®¤è¯
â”œâ”€â”€ client/              # API å®¢æˆ·ç«¯æ¨¡å—
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ bilibili_client.py  # Bç«™ API å®¢æˆ·ç«¯
â”œâ”€â”€ crawler/             # çˆ¬è™«æ¨¡å—
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ spider.py        # Bç«™çˆ¬è™«å®ç°
â”œâ”€â”€ store/               # å­˜å‚¨æ¨¡å—
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ backend.py       # å­˜å‚¨åç«¯
â”œâ”€â”€ proxy/               # ä»£ç†æ¨¡å—ï¼ˆå¯é€‰ï¼‰
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ pool.py          # ä»£ç†æ± 
â”œâ”€â”€ models/              # æ•°æ®æ¨¡å‹æ¨¡å—
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ bilibili.py      # Bç«™æ•°æ®æ¨¡å‹
â”œâ”€â”€ tools/               # å·¥å…·æ¨¡å—
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ sign.py          # WBI ç­¾åå·¥å…·
â”œâ”€â”€ analysis/            # åˆ†ææ¨¡å—
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ report.py        # æŠ¥å‘Šç”Ÿæˆ
â””â”€â”€ main.py              # å…¥å£æ–‡ä»¶
```

## 11.2 é…ç½®æ¨¡å—è®¾è®¡

### é€šç”¨é…ç½®

ä½¿ç”¨ pydantic-settings å®ç°ç±»å‹å®‰å…¨çš„é…ç½®ç®¡ç†ï¼š

```python
# config/settings.py
from pydantic_settings import BaseSettings
from pydantic import Field
from typing import Optional, List
from enum import Enum


class StorageType(str, Enum):
    """å­˜å‚¨ç±»å‹"""
    JSON = "json"
    CSV = "csv"


class LoginType(str, Enum):
    """ç™»å½•ç±»å‹"""
    COOKIE = "cookie"
    QRCODE = "qrcode"


class CrawlerType(str, Enum):
    """çˆ¬å–ç±»å‹"""
    SEARCH = "search"    # å…³é”®è¯æœç´¢
    DETAIL = "detail"    # æŒ‡å®šè§†é¢‘è¯¦æƒ…


class Settings(BaseSettings):
    """é¡¹ç›®é…ç½®"""

    # åŸºç¡€é…ç½®
    app_name: str = "BilibiliCrawler"
    debug: bool = False

    # æµè§ˆå™¨é…ç½®
    browser_headless: bool = False  # Bç«™æ‰«ç ç™»å½•éœ€è¦æ˜¾ç¤ºæµè§ˆå™¨
    browser_timeout: int = 30000
    browser_user_data_dir: Optional[str] = "./browser_data"
    save_login_state: bool = True

    # ç™»å½•é…ç½®
    login_type: LoginType = LoginType.QRCODE
    cookie_str: str = ""

    # çˆ¬è™«é…ç½®
    crawler_type: CrawlerType = CrawlerType.SEARCH
    keywords: str = "Pythonæ•™ç¨‹"  # æœç´¢å…³é”®è¯ï¼Œå¤šä¸ªç”¨é€—å·åˆ†éš”
    specified_id_list: List[str] = []  # æŒ‡å®šè§†é¢‘åˆ—è¡¨
    max_video_count: int = 20
    max_concurrency: int = 3
    crawl_delay_min: float = 1.0
    crawl_delay_max: float = 3.0

    # å­˜å‚¨é…ç½®
    storage_type: StorageType = StorageType.JSON
    storage_output_dir: str = "./output"

    class Config:
        env_file = ".env"
        env_prefix = "CRAWLER_"


# å…¨å±€é…ç½®å®ä¾‹
settings = Settings()
```

### Bç«™ç‰¹å®šé…ç½®

```python
# config/bilibili_config.py
"""Bç«™ API é…ç½®"""

# API åœ°å€
SEARCH_URL = "https://api.bilibili.com/x/web-interface/wbi/search/type"
VIDEO_INFO_URL = "https://api.bilibili.com/x/web-interface/view"
NAV_URL = "https://api.bilibili.com/x/web-interface/nav"

# è¯·æ±‚é…ç½®
SEARCH_PAGE_SIZE = 20
REQUEST_TIMEOUT = 30

# é»˜è®¤è¯·æ±‚å¤´
DEFAULT_HEADERS = {
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
                  "AppleWebKit/537.36 (KHTML, like Gecko) "
                  "Chrome/120.0.0.0 Safari/537.36",
    "Referer": "https://www.bilibili.com",
    "Origin": "https://www.bilibili.com",
}

# WBI ç­¾åå¯†é’¥æ··æ·†è¡¨
WBI_MIXIN_KEY_ENC_TAB = [
    46, 47, 18, 2, 53, 8, 23, 32, 15, 50, 10, 31, 58, 3, 45, 35,
    27, 43, 5, 49, 33, 9, 42, 19, 29, 28, 14, 39, 12, 38, 41, 13,
    37, 48, 7, 16, 24, 55, 40, 61, 26, 17, 0, 1, 60, 51, 30, 4,
    22, 25, 54, 21, 56, 59, 6, 63, 57, 62, 11, 36, 20, 34, 44, 52,
]

# ç™»å½•ç›¸å…³
LOGIN_BUTTON_SELECTOR = "xpath=//div[@class='right-entry__outside go-login-btn']//div"
QRCODE_SELECTOR = "//div[@class='login-scan-box']//img"
LOGIN_COOKIE_KEYS = ["SESSDATA", "DedeUserID", "bili_jct"]
```

## 11.3 WBI ç­¾åç®—æ³•

Bç«™ä½¿ç”¨ WBI ç­¾åä¿æŠ¤ API è¯·æ±‚ï¼Œéœ€è¦å®ç°ç­¾åç®—æ³•ã€‚

### WBI ç­¾ååŸç†

WBIï¼ˆWeb Bilibili Interfaceï¼‰ç­¾åæ˜¯ Bç«™ç”¨æ¥ä¿æŠ¤ API æ¥å£çš„ä¸€ç§æœºåˆ¶ï¼Œé˜²æ­¢æ¥å£è¢«æ¶æ„è°ƒç”¨ã€‚

**ä¸ºä»€ä¹ˆéœ€è¦ç­¾åï¼Ÿ**

- é˜²æ­¢è¯·æ±‚è¢«ç¯¡æ”¹
- é˜²æ­¢æ¥å£è¢«æ»¥ç”¨
- å¢åŠ çˆ¬è™«éš¾åº¦

### WBI ç­¾åæµç¨‹å›¾

```mermaid
flowchart TD
    subgraph step1 ["ç¬¬ä¸€æ­¥: è·å–ç­¾åå¯†é’¥"]
        api["è°ƒç”¨ /x/web-interface/nav API"]
        api --> response["å“åº”ä¸­åŒ…å« wbi_img:<br/>{img_url, sub_url}"]
        response --> extract["æå–æ–‡ä»¶åä½œä¸ºå¯†é’¥:<br/>img_key, sub_key"]
    end

    step1 --> step2

    subgraph step2 ["ç¬¬äºŒæ­¥: ç”Ÿæˆç›å€¼ Salt"]
        concat["raw_key = img_key + sub_key"]
        concat --> mixin["ä½¿ç”¨æ··æ·†è¡¨é‡æ’å­—ç¬¦:<br/>WBI_MIXIN_KEY_ENC_TAB"]
        mixin --> salt["salt = é‡æ’åå–å‰32ä½"]
    end

    step2 --> step3

    subgraph step3 ["ç¬¬ä¸‰æ­¥: è®¡ç®—ç­¾å"]
        params["åŸå§‹å‚æ•°:<br/>{keyword, page}"]
        params --> addwts["æ·»åŠ æ—¶é—´æˆ³ wts"]
        addwts --> sort["æŒ‰ key æ’åºå¹¶URLç¼–ç "]
        sort --> append["æ‹¼æ¥ç›å€¼: query + salt"]
        append --> md5["è®¡ç®— MD5 å¾—åˆ° w_rid"]
        md5 --> final["æœ€ç»ˆå‚æ•°:<br/>{...åŸå‚æ•°, wts, w_rid}"]
    end
```

> **ğŸ’¡ å…³äº JS é€†å‘**
>
> ä½ å¯èƒ½ä¼šå¥½å¥‡ï¼šè¿™ä¸ª WBI ç­¾åç®—æ³•æ˜¯æ€ä¹ˆé€†å‘åˆ†æå‡ºæ¥çš„ï¼Ÿæ··æ·†è¡¨ `WBI_MIXIN_KEY_ENC_TAB` åˆæ˜¯ä»å“ªé‡Œæ‰¾åˆ°çš„ï¼Ÿ
>
> åˆ«ç€æ€¥ï¼è¿™éƒ¨åˆ†æ¶‰åŠåˆ° **JavaScript é€†å‘**æŠ€æœ¯ï¼Œæˆ‘ä¼šåœ¨åé¢çš„ **ã€Œé«˜çº§çˆ¬è™« - JS é€†å‘ã€** ç« èŠ‚ä¸­è¯¦ç»†è®²è§£ã€‚å±Šæ—¶ä¼šå¸¦ä½ ä¸€æ­¥æ­¥åˆ†æ Bç«™çš„å‰ç«¯ä»£ç ï¼Œæ‰¾å‡ºç­¾åç®—æ³•çš„å®ç°ç»†èŠ‚ã€‚
>
> æœ¬ç« çš„é‡ç‚¹æ˜¯è®©ä½ ç†è§£**å¦‚ä½•ä½¿ç”¨**è¿™ä¸ªç­¾åç®—æ³•ï¼Œä»¥åŠæ•´ä¸ªé¡¹ç›®çš„å·¥ç¨‹åŒ–æ¶æ„ã€‚ç­¾åç®—æ³•çš„é€†å‘åˆ†æè¿‡ç¨‹ï¼Œæˆ‘ä»¬åé¢å†æ·±å…¥æ¢è®¨ã€‚

### ç­¾åå™¨å®ç°

```python
# tools/sign.py
import hashlib
import time
import urllib.parse
from typing import Dict, Tuple
from functools import reduce

from ..config import bilibili_config


class BilibiliSign:
    """
    Bç«™ WBI ç­¾åå™¨

    WBI ç­¾åç®—æ³•ç”¨äºä¿æŠ¤ Bç«™ API è¯·æ±‚ã€‚
    ç­¾åæµç¨‹ï¼š
    1. ä» wbi_img_urls ä¸­æå– img_key å’Œ sub_key
    2. ä½¿ç”¨æ··æ·†è¡¨ç”Ÿæˆ salt
    3. å¯¹è¯·æ±‚å‚æ•°è¿›è¡Œç­¾å
    """

    def __init__(self, img_key: str, sub_key: str):
        """
        åˆå§‹åŒ–ç­¾åå™¨

        Args:
            img_key: ä» img_url ä¸­æå–çš„å¯†é’¥
            sub_key: ä» sub_url ä¸­æå–çš„å¯†é’¥
        """
        self.img_key = img_key
        self.sub_key = sub_key

    def get_salt(self) -> str:
        """
        ç”Ÿæˆç›å€¼

        é€šè¿‡æ··æ·†è¡¨å¯¹ img_key + sub_key è¿›è¡Œé‡æ’ã€‚
        """
        raw_wbi_key = self.img_key + self.sub_key
        return reduce(
            lambda s, i: s + raw_wbi_key[i],
            bilibili_config.WBI_MIXIN_KEY_ENC_TAB,
            ''
        )[:32]

    def sign(self, req_data: Dict) -> Dict:
        """
        å¯¹è¯·æ±‚å‚æ•°è¿›è¡Œç­¾å

        Args:
            req_data: åŸå§‹è¯·æ±‚å‚æ•°

        Returns:
            Dict: ç­¾ååçš„è¯·æ±‚å‚æ•°ï¼ˆåŒ…å« wts å’Œ w_ridï¼‰
        """
        salt = self.get_salt()

        # æ·»åŠ æ—¶é—´æˆ³
        req_data['wts'] = int(time.time())

        # æŒ‰ key æ’åºå¹¶ç¼–ç 
        params = dict(sorted(req_data.items()))
        query = urllib.parse.urlencode(params)

        # è®¡ç®—ç­¾å
        text_to_sign = query + salt
        w_rid = hashlib.md5(text_to_sign.encode()).hexdigest()

        req_data['w_rid'] = w_rid
        return req_data


def extract_wbi_keys_from_urls(img_url: str, sub_url: str) -> Tuple[str, str]:
    """
    ä» URL ä¸­æå– WBI å¯†é’¥

    Args:
        img_url: wbi_img çš„ img_url
        sub_url: wbi_img çš„ sub_url

    Returns:
        Tuple[str, str]: (img_key, sub_key)
    """
    def extract_key(url: str) -> str:
        # ä» URL ä¸­æå–æ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰
        # ä¾‹å¦‚ï¼šhttps://xxx/bfs/wbi/xxx.png -> xxx
        filename = url.rsplit('/', 1)[-1]
        return filename.split('.')[0]

    return extract_key(img_url), extract_key(sub_url)
```

## 11.4 æ•°æ®æ¨¡å‹å®šä¹‰

ä½¿ç”¨ Pydantic å®šä¹‰è§†é¢‘æ•°æ®æ¨¡å‹ï¼š

```python
# models/bilibili.py
from typing import Optional, List
from datetime import datetime
from pydantic import BaseModel, Field


class BilibiliVideo(BaseModel):
    """Bç«™è§†é¢‘ä¿¡æ¯æ¨¡å‹"""

    # è§†é¢‘æ ‡è¯†
    video_id: str = Field(default="", description="è§†é¢‘ aid")
    bvid: str = Field(default="", description="è§†é¢‘ BV å·")

    # è§†é¢‘ä¿¡æ¯
    title: str = Field(default="", description="è§†é¢‘æ ‡é¢˜")
    desc: str = Field(default="", description="è§†é¢‘æè¿°")
    cover_url: str = Field(default="", description="å°é¢ URL")
    duration: int = Field(default=0, description="æ—¶é•¿ï¼ˆç§’ï¼‰")
    create_time: int = Field(default=0, description="å‘å¸ƒæ—¶é—´æˆ³")

    # UPä¸»ä¿¡æ¯
    user_id: int = Field(default=0, description="UPä¸» ID")
    nickname: str = Field(default="", description="UPä¸»æ˜µç§°")
    avatar: str = Field(default="", description="UPä¸»å¤´åƒ")

    # äº’åŠ¨æ•°æ®
    play_count: int = Field(default=0, description="æ’­æ”¾é‡")
    liked_count: int = Field(default=0, description="ç‚¹èµæ•°")
    coin_count: int = Field(default=0, description="æŠ•å¸æ•°")
    favorite_count: int = Field(default=0, description="æ”¶è—æ•°")
    share_count: int = Field(default=0, description="åˆ†äº«æ•°")
    danmaku_count: int = Field(default=0, description="å¼¹å¹•æ•°")
    comment_count: int = Field(default=0, description="è¯„è®ºæ•°")

    # çˆ¬å–ä¿¡æ¯
    source_keyword: str = Field(default="", description="æœç´¢å…³é”®è¯")
    crawl_time: str = Field(
        default_factory=lambda: datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        description="çˆ¬å–æ—¶é—´"
    )

    @classmethod
    def from_api_response(cls, data: dict, source_keyword: str = "") -> "BilibiliVideo":
        """ä»è§†é¢‘è¯¦æƒ… API å“åº”æ„å»ºæ¨¡å‹"""
        stat = data.get("stat", {})
        owner = data.get("owner", {})

        return cls(
            video_id=str(data.get("aid", "")),
            bvid=data.get("bvid", ""),
            title=data.get("title", ""),
            desc=data.get("desc", ""),
            cover_url=data.get("pic", ""),
            duration=data.get("duration", 0),
            create_time=data.get("pubdate", 0),
            user_id=owner.get("mid", 0),
            nickname=owner.get("name", ""),
            avatar=owner.get("face", ""),
            play_count=stat.get("view", 0),
            liked_count=stat.get("like", 0),
            coin_count=stat.get("coin", 0),
            favorite_count=stat.get("favorite", 0),
            share_count=stat.get("share", 0),
            danmaku_count=stat.get("danmaku", 0),
            comment_count=stat.get("reply", 0),
            source_keyword=source_keyword,
        )

    @classmethod
    def from_search_result(cls, data: dict, keyword: str = "") -> "BilibiliVideo":
        """ä»æœç´¢ç»“æœæ„å»ºæ¨¡å‹"""
        return cls(
            video_id=str(data.get("aid", "")),
            bvid=data.get("bvid", ""),
            title=data.get("title", "").replace("<em class=\"keyword\">", "").replace("</em>", ""),
            desc=data.get("description", ""),
            cover_url="https:" + data.get("pic", "") if data.get("pic", "").startswith("//") else data.get("pic", ""),
            duration=data.get("duration", 0) if isinstance(data.get("duration"), int) else 0,
            user_id=data.get("mid", 0),
            nickname=data.get("author", ""),
            avatar=data.get("upic", ""),
            play_count=data.get("play", 0),
            liked_count=data.get("like", 0),
            danmaku_count=data.get("danmaku", 0),
            source_keyword=keyword,
        )

    def to_dict(self) -> dict:
        """è½¬æ¢ä¸ºå­—å…¸"""
        return self.model_dump()
```

## 11.5 ç™»å½•è®¤è¯æ¨¡å—

ç™»å½•è®¤è¯æ˜¯çˆ¬è™«è·å–å®Œæ•´æ•°æ®çš„å…³é”®æ­¥éª¤ã€‚Bç«™å¯¹æœªç™»å½•ç”¨æˆ·æœ‰å¾ˆå¤šæ•°æ®é™åˆ¶ï¼Œç™»å½•åå¯ä»¥è·å–æ›´å¤šä¿¡æ¯ã€‚

### ä¸ºä»€ä¹ˆéœ€è¦ç™»å½•ï¼Ÿ

| æ•°æ®é¡¹ | æœªç™»å½• | å·²ç™»å½• |
|--------|--------|--------|
| æœç´¢ç»“æœ | æœ‰é™åˆ¶ | å®Œæ•´ |
| è§†é¢‘è¯¦æƒ… | åŸºç¡€ä¿¡æ¯ | å®Œæ•´ä¿¡æ¯ |
| ç”¨æˆ·æ•°æ® | éƒ¨åˆ†éšè— | å¯è§ |
| APIè°ƒç”¨é¢‘ç‡ | ä¸¥æ ¼é™åˆ¶ | ç›¸å¯¹å®½æ¾ |

### ç™»å½•æ–¹å¼å¯¹æ¯”

| æ–¹å¼ | ä¼˜ç‚¹ | ç¼ºç‚¹ | é€‚ç”¨åœºæ™¯ |
|------|------|------|----------|
| æ‰«ç ç™»å½• | å®‰å…¨ã€æ— éœ€å¤„ç†å¤æ‚é€»è¾‘ | éœ€è¦æ‰‹æœºAPPé…åˆ | é¦–æ¬¡ç™»å½•ã€å¼€å‘è°ƒè¯• |
| Cookieç™»å½• | å¿«é€Ÿã€å¯è‡ªåŠ¨åŒ– | Cookieä¼šè¿‡æœŸ | æ‰¹é‡éƒ¨ç½²ã€å®šæ—¶ä»»åŠ¡ |

### æ‰«ç ç™»å½•æµç¨‹å›¾

```mermaid
sequenceDiagram
    participant PC as PCæµè§ˆå™¨
    participant Server as Bç«™æœåŠ¡å™¨
    participant APP as Bç«™APP

    PC->>Server: 1. è®¿é—®Bç«™é¦–é¡µ
    PC->>Server: 2. ç‚¹å‡»ç™»å½•æŒ‰é’®
    PC->>Server: 3. è¯·æ±‚äºŒç»´ç +UUID
    Server-->>PC: 4. è¿”å›äºŒç»´ç å›¾ç‰‡
    Note over PC: 5. æ˜¾ç¤ºäºŒç»´ç 

    loop è½®è¯¢ç™»å½•çŠ¶æ€
        PC->>Server: 6. æ£€æŸ¥ç™»å½•çŠ¶æ€
        APP->>Server: 7. ç”¨æˆ·æ‰«æäºŒç»´ç 
        Server-->>PC: 8. çŠ¶æ€: å·²æ‰«æ
        PC->>Server: 9. ç»§ç»­è½®è¯¢
        APP->>Server: 10. ç”¨æˆ·ç‚¹å‡»ç¡®è®¤
        Server-->>PC: 11. è¿”å›ç™»å½•å‡­è¯(Set-Cookie)
    end

    Note over PC: 12. ä¿å­˜Cookie<br/>ç™»å½•æˆåŠŸ!
```

### Cookie ç™»å½•æµç¨‹å›¾

```mermaid
flowchart LR
    subgraph input [ç”¨æˆ·è¾“å…¥]
        cookie["ç”¨æˆ·æä¾›Cookie<br/>(ä»æµè§ˆå™¨å¤åˆ¶)"]
    end

    subgraph process [å¤„ç†æµç¨‹]
        parse["è§£æCookieå­—ç¬¦ä¸²<br/>æå–é”®å€¼å¯¹"]
        inject["æ³¨å…¥åˆ°BrowserContext<br/>add_cookies()"]
        visit["è®¿é—®Bç«™é¦–é¡µ<br/>åŠ è½½é¡µé¢"]
        check{"æ£€æŸ¥å…³é”®Cookie<br/>SESSDATAå­˜åœ¨?"}
    end

    subgraph result [ç»“æœ]
        success["ç™»å½•æˆåŠŸ!<br/>å¼€å§‹çˆ¬å–"]
        fail["Cookieå·²è¿‡æœŸ<br/>éœ€è¦é‡æ–°ç™»å½•"]
    end

    cookie --> parse --> inject --> visit --> check
    check -->|å­˜åœ¨| success
    check -->|ä¸å­˜åœ¨| fail
```

**å…³é”®Cookieè¯´æ˜ï¼š**

| Cookieåç§° | è¯´æ˜ |
|-----------|------|
| SESSDATA | ä¼šè¯å‡­è¯ï¼Œæœ€é‡è¦çš„ç™»å½•æ ‡è¯† |
| DedeUserID | ç”¨æˆ·ID |
| bili_jct | CSRF Tokenï¼ŒæŸäº›æ“ä½œéœ€è¦ |

### ç™»å½•å®ç°ä»£ç 

```python
# login/auth.py
import asyncio
import base64
from abc import ABC, abstractmethod
from pathlib import Path
from typing import Optional, List, Dict
from loguru import logger

from playwright.async_api import BrowserContext, Page

# ç™»å½•ç›¸å…³å¸¸é‡
BILIBILI_URL = "https://www.bilibili.com"
LOGIN_BUTTON_SELECTOR = "xpath=//div[@class='right-entry__outside go-login-btn']//div"
QRCODE_SELECTOR = "//div[@class='login-scan-box']//img"


class BilibiliLogin:
    """
    Bç«™ç™»å½•ç±»

    æ”¯æŒæ‰«ç ç™»å½•å’Œ Cookie ç™»å½•ä¸¤ç§æ–¹å¼ã€‚
    """

    def __init__(
        self,
        login_type: str,
        browser_context: BrowserContext,
        context_page: Page,
        cookie_str: str = "",
    ):
        self.login_type = login_type
        self.browser_context = browser_context
        self.context_page = context_page
        self.cookie_str = cookie_str

    async def begin(self) -> bool:
        """å¼€å§‹ç™»å½•æµç¨‹"""
        logger.info(f"[BilibiliLogin] å¼€å§‹ç™»å½•ï¼Œæ–¹å¼: {self.login_type}")

        if self.login_type == "qrcode":
            return await self.login_by_qrcode()
        elif self.login_type == "cookie":
            return await self.login_by_cookies()
        else:
            logger.error(f"[BilibiliLogin] ä¸æ”¯æŒçš„ç™»å½•ç±»å‹: {self.login_type}")
            return False

    async def login_by_qrcode(self) -> bool:
        """
        æ‰«ç ç™»å½•

        æµç¨‹ï¼š
        1. è®¿é—® Bç«™é¦–é¡µ
        2. ç‚¹å‡»ç™»å½•æŒ‰é’®
        3. è·å–äºŒç»´ç å›¾ç‰‡å¹¶æ˜¾ç¤º
        4. ç­‰å¾…ç”¨æˆ·æ‰«ç 
        5. æ£€æŸ¥ç™»å½•çŠ¶æ€
        """
        logger.info("[BilibiliLogin] å¼€å§‹æ‰«ç ç™»å½•...")

        try:
            # 1. è®¿é—® Bç«™é¦–é¡µ
            await self.context_page.goto(BILIBILI_URL)
            await asyncio.sleep(2)

            # 2. ç‚¹å‡»ç™»å½•æŒ‰é’®
            try:
                login_button = await self.context_page.wait_for_selector(
                    LOGIN_BUTTON_SELECTOR,
                    timeout=10000
                )
                if login_button:
                    await login_button.click()
                    await asyncio.sleep(1)
            except Exception as e:
                logger.warning(f"[BilibiliLogin] ç‚¹å‡»ç™»å½•æŒ‰é’®å¤±è´¥: {e}")

            # 3. è·å–å¹¶æ˜¾ç¤ºäºŒç»´ç 
            qrcode_img = await self._find_login_qrcode()
            if qrcode_img:
                await self._show_qrcode(qrcode_img)

            # 4. ç­‰å¾…ç™»å½•æˆåŠŸ
            logger.info("[BilibiliLogin] è¯·ä½¿ç”¨ Bç«™ APP æ‰«æäºŒç»´ç ç™»å½•...")
            logger.info("[BilibiliLogin] ç­‰å¾…ç™»å½•æˆåŠŸï¼ˆæœ€é•¿ç­‰å¾… 120 ç§’ï¼‰...")

            for _ in range(120):
                if await self.check_login_state():
                    logger.info("[BilibiliLogin] æ‰«ç ç™»å½•æˆåŠŸï¼")
                    await asyncio.sleep(2)
                    return True
                await asyncio.sleep(1)

            logger.error("[BilibiliLogin] æ‰«ç ç™»å½•è¶…æ—¶")
            return False

        except Exception as e:
            logger.error(f"[BilibiliLogin] æ‰«ç ç™»å½•å¤±è´¥: {e}")
            return False

    async def login_by_cookies(self) -> bool:
        """Cookie ç™»å½•"""
        logger.info("[BilibiliLogin] å¼€å§‹ Cookie ç™»å½•...")

        if not self.cookie_str:
            logger.error("[BilibiliLogin] Cookie å­—ç¬¦ä¸²ä¸ºç©º")
            return False

        try:
            cookies = self._parse_cookie_str(self.cookie_str)
            await self.browser_context.add_cookies(cookies)
            logger.info(f"[BilibiliLogin] æˆåŠŸæ³¨å…¥ {len(cookies)} ä¸ª Cookie")

            await self.context_page.goto(BILIBILI_URL)
            await asyncio.sleep(2)

            if await self.check_login_state():
                logger.info("[BilibiliLogin] Cookie ç™»å½•æˆåŠŸï¼")
                return True
            else:
                logger.error("[BilibiliLogin] Cookie ç™»å½•å¤±è´¥ï¼ŒCookie å¯èƒ½å·²è¿‡æœŸ")
                return False

        except Exception as e:
            logger.error(f"[BilibiliLogin] Cookie ç™»å½•å¤±è´¥: {e}")
            return False

    async def check_login_state(self) -> bool:
        """æ£€æŸ¥ç™»å½•çŠ¶æ€"""
        try:
            cookies = await self.browser_context.cookies()
            cookie_dict = {c['name']: c['value'] for c in cookies}

            for key in ["SESSDATA", "DedeUserID"]:
                if key in cookie_dict and cookie_dict[key]:
                    return True
            return False
        except Exception:
            return False

    async def _find_login_qrcode(self) -> Optional[str]:
        """æŸ¥æ‰¾ç™»å½•äºŒç»´ç """
        try:
            qrcode_element = await self.context_page.wait_for_selector(
                QRCODE_SELECTOR,
                timeout=10000
            )
            if qrcode_element:
                qrcode_src = await qrcode_element.get_attribute("src")
                if qrcode_src and qrcode_src.startswith("data:image"):
                    return qrcode_src.split(",")[1]
            return None
        except Exception as e:
            logger.error(f"[BilibiliLogin] è·å–äºŒç»´ç å¤±è´¥: {e}")
            return None

    async def _show_qrcode(self, qrcode_base64: str):
        """æ˜¾ç¤ºäºŒç»´ç """
        try:
            qrcode_bytes = base64.b64decode(qrcode_base64)
            qrcode_path = Path("qrcode.png")
            with open(qrcode_path, 'wb') as f:
                f.write(qrcode_bytes)
            logger.info(f"[BilibiliLogin] äºŒç»´ç å·²ä¿å­˜åˆ°: {qrcode_path.absolute()}")

            print("\n" + "=" * 60)
            print("  è¯·ä½¿ç”¨ Bç«™ APP æ‰«æäºŒç»´ç ç™»å½•")
            print(f"  äºŒç»´ç æ–‡ä»¶: {qrcode_path.absolute()}")
            print("  ç­‰å¾…ç™»å½•ä¸­...")
            print("=" * 60 + "\n")
        except Exception as e:
            logger.error(f"[BilibiliLogin] æ˜¾ç¤ºäºŒç»´ç å¤±è´¥: {e}")

    def _parse_cookie_str(self, cookie_str: str) -> List[Dict]:
        """è§£æ Cookie å­—ç¬¦ä¸²"""
        cookies = []
        for item in cookie_str.split(";"):
            item = item.strip()
            if not item or "=" not in item:
                continue
            parts = item.split("=", 1)
            name = parts[0].strip()
            value = parts[1].strip() if len(parts) > 1 else ""
            if name:
                cookies.append({
                    "name": name,
                    "value": value,
                    "domain": ".bilibili.com",
                    "path": "/"
                })
        return cookies
```

## 11.6 API å®¢æˆ·ç«¯

API å®¢æˆ·ç«¯æ˜¯çˆ¬è™«ä¸ Bç«™æœåŠ¡å™¨äº¤äº’çš„æ ¸å¿ƒæ¨¡å—ï¼Œè´Ÿè´£å‘é€è¯·æ±‚ã€å¤„ç†ç­¾åã€è§£æå“åº”ã€‚

### å®¢æˆ·ç«¯èŒè´£

```mermaid
graph TB
    subgraph æ ¸å¿ƒåŠŸèƒ½
        cookie["Cookieç®¡ç†<br/>â€¢ ä»æµè§ˆå™¨åŒæ­¥<br/>â€¢ æ³¨å…¥åˆ°è¯·æ±‚å¤´<br/>â€¢ éªŒè¯æœ‰æ•ˆæ€§"]
        wbi["WBIç­¾å<br/>â€¢ è·å–å¯†é’¥<br/>â€¢ å‚æ•°ç­¾å<br/>â€¢ è‡ªåŠ¨åˆ·æ–°"]
        http["HTTPè¯·æ±‚<br/>â€¢ GET/POSTè¯·æ±‚<br/>â€¢ è¶…æ—¶å¤„ç†<br/>â€¢ é”™è¯¯é‡è¯•"]
    end

    subgraph APIæ–¹æ³•
        api["BilibiliClient API<br/>â€¢ search_video_by_keyword<br/>â€¢ get_video_info<br/>â€¢ pong (ç™»å½•æ£€æŸ¥)"]
    end

    cookie --> api
    wbi --> api
    http --> api
```

### Bç«™ API åˆ—è¡¨

| API | åœ°å€ | åŠŸèƒ½ | æ˜¯å¦éœ€è¦ç­¾å |
|-----|------|------|-------------|
| ç”¨æˆ·ä¿¡æ¯ | `/x/web-interface/nav` | è·å–ç™»å½•ç”¨æˆ·ä¿¡æ¯å’ŒWBIå¯†é’¥ | å¦ |
| è§†é¢‘æœç´¢ | `/x/web-interface/wbi/search/type` | æŒ‰å…³é”®è¯æœç´¢è§†é¢‘ | **æ˜¯** |
| è§†é¢‘è¯¦æƒ… | `/x/web-interface/view` | è·å–è§†é¢‘å®Œæ•´ä¿¡æ¯ | å¦ |

### å®¢æˆ·ç«¯å®ç°ä»£ç 

```python
# client/bilibili_client.py
import json
from typing import Dict, Optional, List
from loguru import logger
import httpx

from playwright.async_api import BrowserContext, Page

from ..tools.sign import BilibiliSign, extract_wbi_keys_from_urls
from ..models.bilibili import BilibiliVideo
from ..config import bilibili_config


class BilibiliClient:
    """
    Bç«™ API å®¢æˆ·ç«¯

    å°è£… Bç«™çš„ API è¯·æ±‚ï¼Œæ”¯æŒ WBI ç­¾åã€‚
    """

    def __init__(self):
        self.headers = bilibili_config.DEFAULT_HEADERS.copy()
        self.cookie_dict: Dict[str, str] = {}
        self._signer: Optional[BilibiliSign] = None
        self._timeout = bilibili_config.REQUEST_TIMEOUT

    async def update_cookies(self, browser_context: BrowserContext):
        """ä»æµè§ˆå™¨ä¸Šä¸‹æ–‡æ›´æ–° Cookie"""
        cookies = await browser_context.cookies()
        cookie_str = "; ".join([f"{c['name']}={c['value']}" for c in cookies])
        self.headers["Cookie"] = cookie_str
        self.cookie_dict = {c['name']: c['value'] for c in cookies}
        logger.info(f"[BilibiliClient] æ›´æ–°äº† {len(cookies)} ä¸ª Cookie")

    async def init_wbi_sign(self, page: Page):
        """
        åˆå§‹åŒ– WBI ç­¾åå™¨

        ä»æµè§ˆå™¨çš„ localStorage ä¸­è·å– WBI å¯†é’¥ã€‚
        """
        try:
            wbi_img_urls = await page.evaluate("""
                () => {
                    return localStorage.getItem('wbi_img_urls');
                }
            """)

            if not wbi_img_urls:
                logger.warning("[BilibiliClient] æœªæ‰¾åˆ° wbi_img_urlsï¼Œå°è¯•ä» API è·å–")
                await self._fetch_wbi_keys()
                return

            wbi_data = json.loads(wbi_img_urls)
            img_url = wbi_data.get("imgUrl", "")
            sub_url = wbi_data.get("subUrl", "")

            if img_url and sub_url:
                img_key, sub_key = extract_wbi_keys_from_urls(img_url, sub_url)
                self._signer = BilibiliSign(img_key, sub_key)
                logger.info("[BilibiliClient] WBI ç­¾åå™¨åˆå§‹åŒ–æˆåŠŸ")
            else:
                await self._fetch_wbi_keys()

        except Exception as e:
            logger.error(f"[BilibiliClient] åˆå§‹åŒ– WBI ç­¾åå™¨å¤±è´¥: {e}")
            await self._fetch_wbi_keys()

    async def _fetch_wbi_keys(self):
        """ä» API è·å– WBI å¯†é’¥ï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰"""
        try:
            async with httpx.AsyncClient(timeout=self._timeout) as client:
                response = await client.get(
                    "https://api.bilibili.com/x/web-interface/nav",
                    headers=self.headers
                )
                data = response.json()

                if data.get("code") == 0:
                    wbi_img = data.get("data", {}).get("wbi_img", {})
                    img_url = wbi_img.get("img_url", "")
                    sub_url = wbi_img.get("sub_url", "")

                    if img_url and sub_url:
                        img_key, sub_key = extract_wbi_keys_from_urls(img_url, sub_url)
                        self._signer = BilibiliSign(img_key, sub_key)
                        logger.info("[BilibiliClient] ä» API è·å– WBI å¯†é’¥æˆåŠŸ")
                        return

            logger.error("[BilibiliClient] æ— æ³•è·å– WBI å¯†é’¥")

        except Exception as e:
            logger.error(f"[BilibiliClient] è·å– WBI å¯†é’¥å¤±è´¥: {e}")

    async def _request(
        self,
        method: str,
        url: str,
        params: Optional[Dict] = None,
        enable_sign: bool = False
    ) -> Optional[Dict]:
        """å‘é€ HTTP è¯·æ±‚"""
        try:
            if enable_sign and self._signer and params:
                params = self._signer.sign(params)

            async with httpx.AsyncClient(timeout=self._timeout) as client:
                if method.upper() == "GET":
                    response = await client.get(url, params=params, headers=self.headers)
                else:
                    response = await client.post(url, params=params, headers=self.headers)

                if response.status_code == 200:
                    return response.json()
                else:
                    logger.error(f"[BilibiliClient] è¯·æ±‚å¤±è´¥: {response.status_code}")
                    return None

        except Exception as e:
            logger.error(f"[BilibiliClient] è¯·æ±‚å‡ºé”™: {e}")
            return None

    async def search_video_by_keyword(
        self,
        keyword: str,
        page: int = 1,
        page_size: int = 20,
    ) -> List[BilibiliVideo]:
        """
        æŒ‰å…³é”®è¯æœç´¢è§†é¢‘

        Args:
            keyword: æœç´¢å…³é”®è¯
            page: é¡µç 
            page_size: æ¯é¡µæ•°é‡

        Returns:
            List[BilibiliVideo]: è§†é¢‘åˆ—è¡¨
        """
        logger.info(f"[BilibiliClient] æœç´¢è§†é¢‘: {keyword}, ç¬¬ {page} é¡µ")

        params = {
            "keyword": keyword,
            "search_type": "video",
            "page": page,
            "page_size": page_size,
        }

        data = await self._request(
            "GET",
            bilibili_config.SEARCH_URL,
            params=params,
            enable_sign=True
        )

        if not data or data.get("code") != 0:
            logger.error(f"[BilibiliClient] æœç´¢å¤±è´¥: {data.get('message') if data else 'No response'}")
            return []

        result = data.get("data", {})
        video_list = result.get("result", [])

        videos = []
        for item in video_list:
            try:
                video = BilibiliVideo.from_search_result(item, keyword)
                videos.append(video)
            except Exception as e:
                logger.debug(f"[BilibiliClient] è§£æè§†é¢‘å¤±è´¥: {e}")

        logger.info(f"[BilibiliClient] æœç´¢åˆ° {len(videos)} ä¸ªè§†é¢‘")
        return videos

    async def get_video_info(
        self,
        aid: Optional[str] = None,
        bvid: Optional[str] = None
    ) -> Optional[BilibiliVideo]:
        """
        è·å–è§†é¢‘è¯¦æƒ…

        Args:
            aid: è§†é¢‘ aid
            bvid: è§†é¢‘ BV å·

        Returns:
            BilibiliVideo: è§†é¢‘ä¿¡æ¯
        """
        if not aid and not bvid:
            logger.error("[BilibiliClient] aid å’Œ bvid è‡³å°‘æä¾›ä¸€ä¸ª")
            return None

        params = {}
        if bvid:
            params["bvid"] = bvid
        elif aid:
            params["aid"] = aid

        logger.info(f"[BilibiliClient] è·å–è§†é¢‘è¯¦æƒ…: {bvid or aid}")

        data = await self._request(
            "GET",
            bilibili_config.VIDEO_INFO_URL,
            params=params,
            enable_sign=False
        )

        if not data or data.get("code") != 0:
            logger.error(f"[BilibiliClient] è·å–è§†é¢‘è¯¦æƒ…å¤±è´¥")
            return None

        video_data = data.get("data", {})
        return BilibiliVideo.from_api_response(video_data)

    async def pong(self) -> bool:
        """æ£€æŸ¥ç™»å½•çŠ¶æ€"""
        try:
            data = await self._request(
                "GET",
                "https://api.bilibili.com/x/web-interface/nav",
                enable_sign=False
            )

            if data and data.get("code") == 0:
                user_data = data.get("data", {})
                if user_data.get("isLogin"):
                    username = user_data.get("uname", "æœªçŸ¥ç”¨æˆ·")
                    logger.info(f"[BilibiliClient] å·²ç™»å½•: {username}")
                    return True

            return False
        except Exception:
            return False
```

## 11.7 çˆ¬è™«æ¨¡å—

çˆ¬è™«æ¨¡å—æ˜¯æ•´ä¸ªé¡¹ç›®çš„æ ¸å¿ƒè°ƒåº¦å™¨ï¼Œè´Ÿè´£åè°ƒæµè§ˆå™¨ã€ç™»å½•ã€APIå®¢æˆ·ç«¯ç­‰ç»„ä»¶å®Œæˆæ•°æ®é‡‡é›†ä»»åŠ¡ã€‚

### çˆ¬è™«ç±»è®¾è®¡

```mermaid
graph LR
    subgraph BilibiliCrawler
        subgraph å±æ€§
            attr1["browser_manager"] --> desc1["ç®¡ç†Playwrightæµè§ˆå™¨"]
            attr2["browser_context"] --> desc2["æµè§ˆå™¨ä¸Šä¸‹æ–‡(Cookieå®¹å™¨)"]
            attr3["context_page"] --> desc3["é¡µé¢å®ä¾‹"]
            attr4["bili_client"] --> desc4["APIå®¢æˆ·ç«¯"]
            attr5["_results"] --> desc5["çˆ¬å–ç»“æœåˆ—è¡¨"]
        end

        subgraph æ–¹æ³•
            m1["start()"] --> d1["å¯åŠ¨çˆ¬è™«(ä¸»å…¥å£)"]
            m2["_init_browser()"] --> d2["åˆå§‹åŒ–æµè§ˆå™¨"]
            m3["_do_login()"] --> d3["æ‰§è¡Œç™»å½•"]
            m4["_init_client()"] --> d4["åˆå§‹åŒ–APIå®¢æˆ·ç«¯"]
            m5["search_by_keywords()"] --> d5["å…³é”®è¯æœç´¢"]
            m6["get_specified_videos()"] --> d6["è·å–æŒ‡å®šè§†é¢‘"]
            m7["close()"] --> d7["æ¸…ç†èµ„æº"]
        end
    end
```

### ä¸¤ç§çˆ¬å–æ¨¡å¼

#### SEARCH æ¨¡å¼ï¼ˆå…³é”®è¯æœç´¢è§†é¢‘ï¼‰

```mermaid
flowchart LR
    subgraph input [å…³é”®è¯åˆ—è¡¨]
        k1["Python"]
        k2["æ•™ç¨‹"]
        k3["æ•°æ®åˆ†æ"]
    end

    subgraph search [æœç´¢åˆ†é¡µ]
        p1["ç¬¬1é¡µ"]
        p2["ç¬¬2é¡µ"]
        p3["..."]
    end

    subgraph result [è§†é¢‘åˆ—è¡¨]
        v1["BV1xxx"]
        v2["BV2xxx"]
        v3["BV3xxx"]
    end

    detail["è·å–æ¯ä¸ªè§†é¢‘è¯¦æƒ…<br/>(å®Œæ•´æ’­æ”¾é‡ç­‰)"]

    input --> search --> result --> detail
```

#### DETAIL æ¨¡å¼ï¼ˆè·å–æŒ‡å®šè§†é¢‘è¯¦æƒ…ï¼‰

```mermaid
flowchart LR
    subgraph input [æŒ‡å®šBVå·åˆ—è¡¨]
        bv1["BV1abc"]
        bv2["BV2def"]
        bv3["BV3ghi"]
    end

    api["éå†åˆ—è¡¨<br/>é€ä¸ªè°ƒç”¨è¯¦æƒ…API"]

    input --> api
```

### åçˆ¬ç­–ç•¥

ä¸ºäº†é¿å…è¢« Bç«™ å°ç¦ï¼Œçˆ¬è™«é‡‡ç”¨äº†ä»¥ä¸‹ç­–ç•¥ï¼š

| ç­–ç•¥ | å®ç°æ–¹å¼ | é…ç½®é¡¹ |
|------|----------|--------|
| éšæœºå»¶è¿Ÿ | æ¯æ¬¡è¯·æ±‚åéšæœºç­‰å¾… 1-3 ç§’ | `crawl_delay_min`, `crawl_delay_max` |
| é¢‘ç‡æ§åˆ¶ | é™åˆ¶æœ€å¤§çˆ¬å–æ•°é‡ | `max_video_count` |
| ç™»å½•æ€ | ä½¿ç”¨çœŸå®ç™»å½•Cookie | `login_type` |
| å®Œæ•´è¯·æ±‚å¤´ | User-Agentã€Refererç­‰ | `DEFAULT_HEADERS` |

### çˆ¬è™«å®ç°ä»£ç 

```python
# crawler/spider.py
import asyncio
import random
from typing import List, Optional
from loguru import logger

from playwright.async_api import BrowserContext, Page

from ..config import settings, CrawlerType
from ..core.browser import BrowserManager
from ..login.auth import BilibiliLogin
from ..client.bilibili_client import BilibiliClient
from ..models.bilibili import BilibiliVideo


class BilibiliCrawler:
    """
    Bç«™çˆ¬è™«ç±»

    æ•´åˆæµè§ˆå™¨ç®¡ç†ã€ç™»å½•è®¤è¯ã€APIå®¢æˆ·ç«¯ï¼Œå®ç°å®Œæ•´çš„çˆ¬å–æµç¨‹ã€‚
    """

    def __init__(self):
        self.browser_manager: Optional[BrowserManager] = None
        self.browser_context: Optional[BrowserContext] = None
        self.context_page: Optional[Page] = None
        self.bili_client: Optional[BilibiliClient] = None
        self._results: List[BilibiliVideo] = []

        # é…ç½®
        self.max_video_count = settings.max_video_count
        self.delay_min = settings.crawl_delay_min
        self.delay_max = settings.crawl_delay_max

    async def start(self) -> List[BilibiliVideo]:
        """
        å¯åŠ¨çˆ¬è™«

        å®Œæ•´æµç¨‹ï¼š
        1. å¯åŠ¨æµè§ˆå™¨
        2. æ‰§è¡Œç™»å½•
        3. åˆå§‹åŒ– API å®¢æˆ·ç«¯
        4. æ ¹æ®é…ç½®æ‰§è¡Œçˆ¬å–
        5. å…³é—­æµè§ˆå™¨
        """
        logger.info(f"[BilibiliCrawler] å¯åŠ¨çˆ¬è™«ï¼Œç±»å‹: {settings.crawler_type}")

        try:
            # 1. å¯åŠ¨æµè§ˆå™¨
            await self._init_browser()

            # 2. æ‰§è¡Œç™»å½•
            login_success = await self._do_login()
            if not login_success:
                logger.error("[BilibiliCrawler] ç™»å½•å¤±è´¥ï¼Œé€€å‡º")
                return []

            # 3. åˆå§‹åŒ– API å®¢æˆ·ç«¯
            await self._init_client()

            # 4. æ ¹æ®é…ç½®æ‰§è¡Œçˆ¬å–
            if settings.crawler_type == CrawlerType.SEARCH:
                await self.search_by_keywords()
            elif settings.crawler_type == CrawlerType.DETAIL:
                await self.get_specified_videos()

            logger.info(f"[BilibiliCrawler] çˆ¬å–å®Œæˆï¼Œå…± {len(self._results)} ä¸ªè§†é¢‘")
            return self._results

        except Exception as e:
            logger.exception(f"[BilibiliCrawler] çˆ¬å–å‡ºé”™: {e}")
            return self._results

        finally:
            await self.close()

    async def _init_browser(self):
        """åˆå§‹åŒ–æµè§ˆå™¨"""
        logger.info("[BilibiliCrawler] åˆå§‹åŒ–æµè§ˆå™¨...")

        self.browser_manager = BrowserManager(
            headless=settings.browser_headless,
            timeout=settings.browser_timeout,
            user_data_dir=settings.browser_user_data_dir if settings.save_login_state else None
        )

        self.browser_context = await self.browser_manager.start()
        self.context_page = await self.browser_manager.new_page()

    async def _do_login(self) -> bool:
        """æ‰§è¡Œç™»å½•"""
        self.bili_client = BilibiliClient()
        await self.bili_client.update_cookies(self.browser_context)

        if await self.bili_client.pong():
            logger.info("[BilibiliCrawler] å·²æœ‰ç™»å½•çŠ¶æ€ï¼Œè·³è¿‡ç™»å½•")
            return True

        login = BilibiliLogin(
            login_type=settings.login_type.value,
            browser_context=self.browser_context,
            context_page=self.context_page,
            cookie_str=settings.cookie_str
        )

        success = await login.begin()

        if success:
            await self.bili_client.update_cookies(self.browser_context)

        return success

    async def _init_client(self):
        """åˆå§‹åŒ– API å®¢æˆ·ç«¯"""
        await self.bili_client.init_wbi_sign(self.context_page)

    async def search_by_keywords(self) -> List[BilibiliVideo]:
        """æŒ‰å…³é”®è¯æœç´¢è§†é¢‘"""
        keywords = [kw.strip() for kw in settings.keywords.split(",") if kw.strip()]

        if not keywords:
            logger.warning("[BilibiliCrawler] æœªé…ç½®æœç´¢å…³é”®è¯")
            return []

        logger.info(f"[BilibiliCrawler] å¼€å§‹æœç´¢ï¼Œå…³é”®è¯: {keywords}")

        for keyword in keywords:
            await self._search_single_keyword(keyword)
            if len(self._results) >= self.max_video_count:
                break

        return self._results

    async def _search_single_keyword(self, keyword: str):
        """æœç´¢å•ä¸ªå…³é”®è¯"""
        page = 1

        while len(self._results) < self.max_video_count:
            logger.info(f"[BilibiliCrawler] æœç´¢ '{keyword}'ï¼Œç¬¬ {page} é¡µ")

            videos = await self.bili_client.search_video_by_keyword(
                keyword=keyword,
                page=page,
            )

            if not videos:
                break

            for video in videos:
                if len(self._results) >= self.max_video_count:
                    break

                # è·å–å®Œæ•´è§†é¢‘è¯¦æƒ…
                video_detail = await self.bili_client.get_video_info(bvid=video.bvid)
                if video_detail:
                    video_detail.source_keyword = keyword
                    self._results.append(video_detail)
                    logger.info(f"[BilibiliCrawler] è·å–è§†é¢‘: {video_detail.title[:30]}...")
                else:
                    self._results.append(video)

                await self._random_delay()

            page += 1
            if page > 50:
                break

    async def get_specified_videos(self) -> List[BilibiliVideo]:
        """è·å–æŒ‡å®šè§†é¢‘åˆ—è¡¨çš„è¯¦æƒ…"""
        video_list = settings.specified_id_list

        if not video_list:
            logger.warning("[BilibiliCrawler] æœªé…ç½®æŒ‡å®šè§†é¢‘åˆ—è¡¨")
            return []

        logger.info(f"[BilibiliCrawler] è·å– {len(video_list)} ä¸ªæŒ‡å®šè§†é¢‘")

        for video_id in video_list:
            if len(self._results) >= self.max_video_count:
                break

            video = await self.bili_client.get_video_info(bvid=video_id)
            if video:
                self._results.append(video)
                logger.info(f"[BilibiliCrawler] è·å–è§†é¢‘: {video.title[:30]}...")

            await self._random_delay()

        return self._results

    async def _random_delay(self):
        """éšæœºå»¶è¿Ÿ"""
        delay = random.uniform(self.delay_min, self.delay_max)
        await asyncio.sleep(delay)

    async def close(self):
        """å…³é—­æµè§ˆå™¨"""
        if self.browser_manager:
            await self.browser_manager.close()
```

## 11.8 æ•°æ®å­˜å‚¨æ¨¡å—

æ•°æ®å­˜å‚¨æ¨¡å—è´Ÿè´£å°†çˆ¬å–åˆ°çš„æ•°æ®æŒä¹…åŒ–ä¿å­˜ï¼Œæ”¯æŒå¤šç§å­˜å‚¨æ ¼å¼ã€‚

### å­˜å‚¨æ¶æ„è®¾è®¡

é‡‡ç”¨**ç­–ç•¥æ¨¡å¼**è®¾è®¡ï¼Œæ–¹ä¾¿æ‰©å±•æ–°çš„å­˜å‚¨æ–¹å¼ï¼š

```mermaid
graph TB
    subgraph manager [å­˜å‚¨ç®¡ç†å™¨]
        sm["StorageManager<br/>â€¢ save(data)<br/>â€¢ load()<br/>â€¢ filepath"]
    end

    subgraph base [æŠ½è±¡åŸºç±»]
        bs["BaseStorage (ABC)<br/>â€¢ save() æŠ½è±¡æ–¹æ³•<br/>â€¢ load() æŠ½è±¡æ–¹æ³•"]
    end

    subgraph impl [å…·ä½“å®ç°]
        json["JSONStorage<br/>â€¢ ä¿å­˜ä¸ºJSONæ–‡ä»¶<br/>â€¢ ä¿æŒæ•°æ®ç»“æ„<br/>â€¢ ä¾¿äºç¨‹åºå¤„ç†"]
        csv["CSVStorage<br/>â€¢ ä¿å­˜ä¸ºCSVæ–‡ä»¶<br/>â€¢ é€‚åˆExcelæ‰“å¼€<br/>â€¢ ä¾¿äºæ•°æ®åˆ†æ"]
    end

    sm -->|æ ¹æ®é…ç½®é€‰æ‹©| json
    sm -->|æ ¹æ®é…ç½®é€‰æ‹©| csv
    json -->|ç»§æ‰¿| bs
    csv -->|ç»§æ‰¿| bs
```

### å­˜å‚¨æ ¼å¼å¯¹æ¯”

| æ ¼å¼ | ä¼˜ç‚¹ | ç¼ºç‚¹ | é€‚ç”¨åœºæ™¯ |
|------|------|------|----------|
| JSON | ä¿æŒåµŒå¥—ç»“æ„ã€ç¨‹åºæ˜“è¯»å– | æ–‡ä»¶è¾ƒå¤§ã€ä¸ä¾¿äººå·¥æŸ¥çœ‹ | åç»­ç¨‹åºå¤„ç†ã€APIæ¥å£ |
| CSV | Excelå¯æ‰“å¼€ã€ä¾¿äºåˆ†æ | æ— æ³•ä¿å­˜åµŒå¥—ç»“æ„ | æ•°æ®åˆ†æã€æŠ¥è¡¨åˆ¶ä½œ |

### å­˜å‚¨å®ç°ä»£ç 

```python
# store/backend.py
import json
import csv
from abc import ABC, abstractmethod
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any
from loguru import logger


class BaseStorage(ABC):
    """å­˜å‚¨åŸºç±»"""

    @abstractmethod
    async def save(self, data: List[Dict]) -> bool:
        pass

    @abstractmethod
    async def load(self) -> List[Dict]:
        pass


class JSONStorage(BaseStorage):
    """JSON å­˜å‚¨"""

    def __init__(self, output_dir: str, filename: str = None):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)

        if filename:
            self.filepath = self.output_dir / filename
        else:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            self.filepath = self.output_dir / f"data_{timestamp}.json"

    async def save(self, data: List[Dict]) -> bool:
        try:
            with open(self.filepath, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            logger.info(f"æ•°æ®å·²ä¿å­˜åˆ°: {self.filepath} ({len(data)} æ¡)")
            return True
        except Exception as e:
            logger.error(f"ä¿å­˜å¤±è´¥: {e}")
            return False

    async def load(self) -> List[Dict]:
        if not self.filepath.exists():
            return []
        try:
            with open(self.filepath, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            logger.error(f"åŠ è½½å¤±è´¥: {e}")
            return []


class CSVStorage(BaseStorage):
    """CSV å­˜å‚¨"""

    def __init__(self, output_dir: str, filename: str = None, fields: List[str] = None):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)

        if filename:
            self.filepath = self.output_dir / filename
        else:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            self.filepath = self.output_dir / f"data_{timestamp}.csv"

        self.fields = fields

    async def save(self, data: List[Dict]) -> bool:
        if not data:
            logger.warning("æ²¡æœ‰æ•°æ®éœ€è¦ä¿å­˜")
            return True

        try:
            fields = self.fields or list(data[0].keys())

            with open(self.filepath, 'w', encoding='utf-8-sig', newline='') as f:
                writer = csv.DictWriter(f, fieldnames=fields, extrasaction='ignore')
                writer.writeheader()
                writer.writerows(data)

            logger.info(f"æ•°æ®å·²ä¿å­˜åˆ°: {self.filepath} ({len(data)} æ¡)")
            return True
        except Exception as e:
            logger.error(f"ä¿å­˜å¤±è´¥: {e}")
            return False

    async def load(self) -> List[Dict]:
        if not self.filepath.exists():
            return []
        try:
            with open(self.filepath, 'r', encoding='utf-8-sig') as f:
                reader = csv.DictReader(f)
                return list(reader)
        except Exception as e:
            logger.error(f"åŠ è½½å¤±è´¥: {e}")
            return []


class StorageManager:
    """å­˜å‚¨ç®¡ç†å™¨"""

    def __init__(self, storage_type: str, output_dir: str, **kwargs):
        self.output_dir = output_dir

        if storage_type == 'json':
            self._storage = JSONStorage(output_dir, **kwargs)
        elif storage_type == 'csv':
            self._storage = CSVStorage(output_dir, **kwargs)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„å­˜å‚¨ç±»å‹: {storage_type}")

    async def save(self, data: List[Dict]) -> bool:
        return await self._storage.save(data)

    async def load(self) -> List[Dict]:
        return await self._storage.load()

    @property
    def filepath(self) -> Path:
        return self._storage.filepath
```

## 11.9 åˆ†ææŠ¥å‘Šæ¨¡å—

åˆ†ææŠ¥å‘Šæ¨¡å—è´Ÿè´£å¯¹çˆ¬å–çš„æ•°æ®è¿›è¡Œç»Ÿè®¡åˆ†æï¼Œç”Ÿæˆå¯è§†åŒ–æŠ¥å‘Šã€‚

### åˆ†æåŠŸèƒ½æ¦‚è§ˆ

```mermaid
flowchart LR
    subgraph input [è¾“å…¥æ•°æ®]
        videos["BilibiliVideo<br/>å¯¹è±¡åˆ—è¡¨"]
    end

    subgraph process [åˆ†æå¤„ç†]
        stats["è§†é¢‘æŒ‡æ ‡ç»Ÿè®¡<br/>â€¢ æ’­æ”¾é‡ç»Ÿè®¡<br/>â€¢ ç‚¹èµæ•°ç»Ÿè®¡<br/>â€¢ æ”¶è—æ•°ç»Ÿè®¡"]
        top["çƒ­é—¨è§†é¢‘æ’å<br/>TOP 10"]
        up["UPä¸»åˆ†å¸ƒç»Ÿè®¡<br/>è¯é¢‘åˆ†æ<br/>(jiebaåˆ†è¯)"]
    end

    subgraph output [è¾“å‡ºç»“æœ]
        md["Markdown æŠ¥å‘Š<br/>â€¢ æ•°æ®è¡¨æ ¼<br/>â€¢ TOPæ’å<br/>â€¢ UPä¸»åˆ†å¸ƒ"]
        img["è¯äº‘å›¾ç‰‡<br/>(PNG)"]
    end

    videos --> stats --> md
    stats --> top --> img
    top --> up
```

### æŠ¥å‘Šå†…å®¹ç»“æ„

ç”Ÿæˆçš„ Markdown æŠ¥å‘ŠåŒ…å«ä»¥ä¸‹ç« èŠ‚ï¼š

| ç« èŠ‚ | å†…å®¹ | åˆ†æç»´åº¦ |
|------|------|----------|
| è§†é¢‘æŒ‡æ ‡ç»Ÿè®¡ | æ’­æ”¾é‡ã€ç‚¹èµã€æŠ•å¸ç­‰æŒ‡æ ‡çš„æ±‡æ€»ç»Ÿè®¡ | æ€»è®¡ã€å¹³å‡ã€æœ€é«˜ã€æœ€ä½ |
| çƒ­é—¨è§†é¢‘ TOP 10 | æŒ‰æ’­æ”¾é‡æ’åºçš„å‰10ä¸ªè§†é¢‘ | æ ‡é¢˜ã€UPä¸»ã€æ’­æ”¾é‡ã€ç‚¹èµ |
| UPä¸»åˆ†å¸ƒ TOP 10 | å‡ºç°é¢‘ç‡æœ€é«˜çš„UPä¸» | UPä¸»åç§°ã€è§†é¢‘æ•°é‡ |
| æ ‡é¢˜çƒ­è¯ TOP 20 | è§†é¢‘æ ‡é¢˜ä¸­å‡ºç°æœ€å¤šçš„è¯æ±‡ | è¯æ±‡ã€å‡ºç°é¢‘æ¬¡ |
| æ ‡é¢˜è¯äº‘ | å¯è§†åŒ–å±•ç¤ºçƒ­é—¨è¯æ±‡ | è¯äº‘å›¾ç‰‡ |

### å¯é€‰ä¾èµ–è¯´æ˜

åˆ†ææ¨¡å—ä½¿ç”¨äº†å¯é€‰ä¾èµ–ï¼Œå³ä½¿æ²¡æœ‰å®‰è£…ä¹Ÿä¸ä¼šæŠ¥é”™ï¼š

```mermaid
graph TB
    subgraph deps [å¯é€‰ä¾èµ–]
        jieba["jieba<br/>ä¸­æ–‡åˆ†è¯<br/>(è¯é¢‘ç»Ÿè®¡)"]
        wc["wordcloud<br/>è¯äº‘ç”Ÿæˆ<br/>(å¯è§†åŒ–)"]
        pd["pandas<br/>æ•°æ®å¤„ç†<br/>(å¯é€‰)"]
    end

    subgraph fallback [æœªå®‰è£…æ—¶çš„é™çº§ç­–ç•¥]
        note["å¦‚æœæœªå®‰è£…:<br/>â€¢ jieba æœªå®‰è£… - è·³è¿‡è¯é¢‘ç»Ÿè®¡å’Œè¯äº‘ç”Ÿæˆ<br/>â€¢ wordcloud æœªå®‰è£… - è·³è¿‡è¯äº‘ç”Ÿæˆ<br/>â€¢ pandas æœªå®‰è£… - ä½¿ç”¨çº¯Pythonå®ç°ç»Ÿè®¡"]
    end

    jieba --> note
    wc --> note
    pd --> note
```

### åˆ†æå®ç°ä»£ç 

```python
# analysis/report.py
from typing import List, Dict, Union
from datetime import datetime
from collections import Counter
from pathlib import Path
from loguru import logger

# å¯é€‰ä¾èµ–
try:
    import jieba
    HAS_JIEBA = True
except ImportError:
    HAS_JIEBA = False

try:
    from wordcloud import WordCloud
    HAS_WORDCLOUD = True
except ImportError:
    HAS_WORDCLOUD = False


class BilibiliAnalyzer:
    """Bç«™è§†é¢‘æ•°æ®åˆ†æå™¨"""

    STOPWORDS = {
        'çš„', 'æ˜¯', 'åœ¨', 'äº†', 'å’Œ', 'ä¸', 'æˆ–', 'æœ‰', 'ä¸ª', 'äºº',
        'è¿™', 'é‚£', 'å°±', 'éƒ½', 'ä¹Ÿ', 'ä¸º', 'å¯¹', 'åˆ°', 'ä»', 'æŠŠ',
    }

    def __init__(self, videos: List[Union[Dict, any]], output_dir: str = "./output"):
        # è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨
        self.data = []
        for video in videos:
            if hasattr(video, 'to_dict'):
                self.data.append(video.to_dict())
            elif hasattr(video, 'model_dump'):
                self.data.append(video.model_dump())
            elif isinstance(video, dict):
                self.data.append(video)

        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)

    def video_metrics_stats(self) -> Dict:
        """è§†é¢‘æŒ‡æ ‡ç»Ÿè®¡"""
        metrics = {
            'play_count': [],
            'liked_count': [],
            'coin_count': [],
            'favorite_count': [],
            'share_count': [],
            'danmaku_count': [],
            'comment_count': [],
        }

        for item in self.data:
            for key in metrics.keys():
                value = item.get(key, 0) or 0
                metrics[key].append(int(value))

        stats = {}
        for key, values in metrics.items():
            if values:
                stats[key] = {
                    'total': sum(values),
                    'avg': sum(values) / len(values),
                    'max': max(values),
                    'min': min(values),
                }
        return stats

    def top_videos(self, metric: str = 'play_count', top_n: int = 10) -> List[Dict]:
        """è·å–æ’åå‰ N çš„è§†é¢‘"""
        sorted_data = sorted(
            self.data,
            key=lambda x: x.get(metric, 0) or 0,
            reverse=True
        )
        return sorted_data[:top_n]

    def up_distribution(self, top_n: int = 10) -> List[tuple]:
        """UPä¸»åˆ†å¸ƒç»Ÿè®¡"""
        counter = Counter()
        for item in self.data:
            nickname = item.get('nickname', 'æœªçŸ¥UPä¸»')
            if nickname:
                counter[nickname] += 1
        return counter.most_common(top_n)

    def word_frequency(self, text_field: str, top_n: int = 20) -> List[tuple]:
        """è¯é¢‘ç»Ÿè®¡"""
        if not HAS_JIEBA:
            return []

        all_words = []
        for item in self.data:
            text = item.get(text_field, '')
            if text:
                words = jieba.lcut(str(text))
                words = [w for w in words if w not in self.STOPWORDS and len(w) > 1]
                all_words.extend(words)

        return Counter(all_words).most_common(top_n)

    def generate_wordcloud(self, text_field: str, output_file: str = "wordcloud.png") -> str:
        """ç”Ÿæˆè¯äº‘"""
        if not HAS_WORDCLOUD:
            return ""

        word_freq = self.word_frequency(text_field, 200)
        if not word_freq:
            return ""

        wc = WordCloud(
            width=1200,
            height=800,
            background_color='white',
            max_words=200,
        )
        wc.generate_from_frequencies(dict(word_freq))

        output_path = self.output_dir / output_file
        wc.to_file(str(output_path))
        return str(output_path)


class ReportGenerator:
    """æŠ¥å‘Šç”Ÿæˆå™¨"""

    def __init__(self, videos: List, output_dir: str = "./output"):
        self.videos = videos
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.analyzer = BilibiliAnalyzer(videos, output_dir)

    def generate(self, title: str = "Bç«™è§†é¢‘æ•°æ®åˆ†ææŠ¥å‘Š") -> str:
        """ç”Ÿæˆå®Œæ•´åˆ†ææŠ¥å‘Š"""
        lines = []

        # æ ‡é¢˜
        lines.append(f"# {title}")
        lines.append("")
        lines.append(f"> ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        lines.append(f"> æ•°æ®é‡: {len(self.analyzer.data)} æ¡")
        lines.append("")
        lines.append("---")
        lines.append("")

        # 1. è§†é¢‘æŒ‡æ ‡ç»Ÿè®¡
        lines.append("## 1. è§†é¢‘æŒ‡æ ‡ç»Ÿè®¡")
        lines.append("")
        metrics_stats = self.analyzer.video_metrics_stats()
        if metrics_stats:
            lines.append("| æŒ‡æ ‡ | æ€»è®¡ | å¹³å‡ | æœ€é«˜ | æœ€ä½ |")
            lines.append("| --- | ---: | ---: | ---: | ---: |")

            metric_names = {
                'play_count': 'æ’­æ”¾é‡',
                'liked_count': 'ç‚¹èµæ•°',
                'coin_count': 'æŠ•å¸æ•°',
                'favorite_count': 'æ”¶è—æ•°',
                'share_count': 'åˆ†äº«æ•°',
                'danmaku_count': 'å¼¹å¹•æ•°',
                'comment_count': 'è¯„è®ºæ•°',
            }

            for key, name in metric_names.items():
                if key in metrics_stats:
                    stat = metrics_stats[key]
                    lines.append(
                        f"| {name} | {stat['total']:,} | "
                        f"{stat['avg']:,.0f} | {stat['max']:,} | {stat['min']:,} |"
                    )
            lines.append("")

        # 2. çƒ­é—¨è§†é¢‘ TOP 10
        lines.append("## 2. çƒ­é—¨è§†é¢‘ TOP 10")
        lines.append("")
        top_videos = self.analyzer.top_videos('play_count', 10)
        if top_videos:
            lines.append("| æ’å | æ ‡é¢˜ | UPä¸» | æ’­æ”¾é‡ | ç‚¹èµ |")
            lines.append("| --- | --- | --- | ---: | ---: |")
            for i, video in enumerate(top_videos, 1):
                title_short = video.get('title', '')[:30] + '...'
                lines.append(
                    f"| {i} | {title_short} | {video.get('nickname', 'æœªçŸ¥')} | "
                    f"{video.get('play_count', 0):,} | {video.get('liked_count', 0):,} |"
                )
            lines.append("")

        # 3. UPä¸»åˆ†å¸ƒ
        lines.append("## 3. UPä¸»åˆ†å¸ƒ TOP 10")
        lines.append("")
        up_dist = self.analyzer.up_distribution(10)
        if up_dist:
            lines.append("| æ’å | UPä¸» | è§†é¢‘æ•° |")
            lines.append("| --- | --- | ---: |")
            for i, (name, count) in enumerate(up_dist, 1):
                lines.append(f"| {i} | {name} | {count} |")
            lines.append("")

        # 4. æ ‡é¢˜çƒ­è¯
        if HAS_JIEBA:
            lines.append("## 4. æ ‡é¢˜çƒ­è¯ TOP 20")
            lines.append("")
            word_freq = self.analyzer.word_frequency('title', 20)
            if word_freq:
                lines.append("| æ’å | è¯æ±‡ | é¢‘æ¬¡ |")
                lines.append("| --- | --- | ---: |")
                for i, (word, count) in enumerate(word_freq, 1):
                    lines.append(f"| {i} | {word} | {count} |")
                lines.append("")

                if HAS_WORDCLOUD:
                    wordcloud_path = self.analyzer.generate_wordcloud('title', 'title_wordcloud.png')
                    if wordcloud_path:
                        lines.append("### æ ‡é¢˜è¯äº‘")
                        lines.append("")
                        lines.append("![æ ‡é¢˜è¯äº‘](title_wordcloud.png)")
                        lines.append("")

        # ä¿å­˜æŠ¥å‘Š
        report_content = '\n'.join(lines)
        report_path = self.output_dir / "report.md"
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report_content)

        logger.info(f"æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
        return str(report_path)


def generate_report(videos: List, output_dir: str = "./output") -> str:
    """ç”Ÿæˆåˆ†ææŠ¥å‘Šï¼ˆä¾¿æ·å‡½æ•°ï¼‰"""
    generator = ReportGenerator(videos, output_dir)
    return generator.generate()
```

## 11.10 ä¸»ç¨‹åºå…¥å£

æ•´åˆæ‰€æœ‰æ¨¡å—ï¼š

```python
# main.py
import asyncio
import sys
from pathlib import Path
from typing import List
from loguru import logger

# å¯¼å…¥å„æ¨¡å—
from config import settings, CrawlerType
from crawler.spider import BilibiliCrawler
from store.backend import StorageManager
from analysis.report import generate_report
from models.bilibili import BilibiliVideo


def setup_logger():
    """é…ç½®æ—¥å¿—"""
    logger.remove()
    logger.add(
        sys.stderr,
        format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | "
               "<level>{level: <8}</level> | "
               "<cyan>{message}</cyan>",
        level="INFO"
    )
    logger.add(
        "logs/bilibili_{time:YYYY-MM-DD}.log",
        rotation="1 day",
        retention="7 days",
        level="DEBUG",
    )


async def main():
    """ä¸»å‡½æ•°"""

    print("""
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘           Bç«™è§†é¢‘æ•°æ®é‡‡é›†ä¸åˆ†æå·¥å…· v2.0                 â•‘
    â•‘                                                          â•‘
    â•‘  åŠŸèƒ½ï¼š                                                  â•‘
    â•‘  - è§†é¢‘æœç´¢ä¸è¯¦æƒ…è·å–                                    â•‘
    â•‘  - æ‰«ç ç™»å½• / Cookie ç™»å½•                                â•‘
    â•‘  - JSON / CSV æ•°æ®å­˜å‚¨                                   â•‘
    â•‘  - è¯äº‘å’Œç»Ÿè®¡åˆ†ææŠ¥å‘Š                                    â•‘
    â•‘                                                          â•‘
    â•‘  å‚è€ƒé¡¹ç›®ï¼šMediaCrawler                                  â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)

    logger.info(f"å¯åŠ¨ {settings.app_name}")
    logger.info(f"çˆ¬å–ç±»å‹: {settings.crawler_type.value}")
    logger.info(f"ç™»å½•æ–¹å¼: {settings.login_type.value}")
    logger.info(f"æœ€å¤§æ•°é‡: {settings.max_video_count}")

    try:
        # 1. è¿è¡Œçˆ¬è™«
        logger.info("å¼€å§‹çˆ¬å–æ•°æ®...")
        crawler = BilibiliCrawler()
        videos = await crawler.start()
        logger.info(f"çˆ¬å–å®Œæˆ: {len(videos)} æ¡è§†é¢‘")

        if not videos:
            logger.warning("æ²¡æœ‰çˆ¬å–åˆ°æ•°æ®ï¼Œé€€å‡º")
            return

        # 2. ä¿å­˜æ•°æ®
        data = [video.to_dict() for video in videos]
        storage = StorageManager(
            storage_type=settings.storage_type.value,
            output_dir=settings.storage_output_dir
        )
        await storage.save(data)
        logger.info(f"æ•°æ®å·²ä¿å­˜: {storage.filepath}")

        # 3. ç”ŸæˆæŠ¥å‘Š
        report_path = generate_report(videos, settings.storage_output_dir)
        logger.info(f"æŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")

        logger.info("=" * 50)
        logger.info("ä»»åŠ¡å®Œæˆï¼")
        logger.info(f"æ•°æ®æ–‡ä»¶: {storage.filepath}")
        logger.info(f"åˆ†ææŠ¥å‘Š: {report_path}")
        logger.info("=" * 50)

    except KeyboardInterrupt:
        logger.warning("ç”¨æˆ·ä¸­æ–­æ‰§è¡Œ")
    except Exception as e:
        logger.exception(f"æ‰§è¡Œå‡ºé”™: {e}")


if __name__ == "__main__":
    setup_logger()
    Path("logs").mkdir(exist_ok=True)
    Path(settings.storage_output_dir).mkdir(parents=True, exist_ok=True)
    asyncio.run(main())
```

## 11.11 è¿è¡Œä¸æµ‹è¯•

### ä¾èµ–å®‰è£…

```bash
# å®‰è£…ä¾èµ–
pip install playwright httpx pydantic pydantic-settings loguru

# å®‰è£… Playwright æµè§ˆå™¨
playwright install chromium

# å¯é€‰ï¼šæ•°æ®åˆ†æä¾èµ–
pip install pandas jieba wordcloud
```

### é…ç½®ä¿®æ”¹

ä¿®æ”¹ `config/settings.py` ä¸­çš„é…ç½®ï¼š

```python
# çˆ¬å–ç±»å‹
crawler_type = CrawlerType.SEARCH  # æˆ– CrawlerType.DETAIL

# æœç´¢å…³é”®è¯
keywords = "Pythonæ•™ç¨‹,æ•°æ®åˆ†æ"

# æœ€å¤§çˆ¬å–æ•°é‡
max_video_count = 20

# ç™»å½•æ–¹å¼
login_type = LoginType.QRCODE  # é¦–æ¬¡ä½¿ç”¨æ‰«ç ç™»å½•
```

### è¿è¡Œç¨‹åº

```bash
cd æºä»£ç /çˆ¬è™«è¿›é˜¶/11_è¿›é˜¶ç»¼åˆå®æˆ˜é¡¹ç›®
python main.py
```

é¦–æ¬¡è¿è¡Œä¼šå¼¹å‡ºæµè§ˆå™¨çª—å£ï¼Œä½¿ç”¨ Bç«™ APP æ‰«ç ç™»å½•åï¼Œç¨‹åºä¼šè‡ªåŠ¨å¼€å§‹çˆ¬å–ã€‚

## æœ¬ç« å°ç»“

æœ¬ç« æˆ‘ä»¬å®Œæˆäº†ä¸€ä¸ªå®Œæ•´çš„ Bç«™è§†é¢‘æ•°æ®é‡‡é›†ä¸åˆ†æé¡¹ç›®ï¼Œç»¼åˆè¿ç”¨äº†ï¼š

1. **ç™»å½•è®¤è¯**
   - æ‰«ç ç™»å½•
   - Cookie ç™»å½•
   - ç™»å½•çŠ¶æ€æŒä¹…åŒ–

2. **API ç­¾å**
   - WBI ç­¾åç®—æ³•
   - ç­¾åå¯†é’¥è·å–

3. **æ•°æ®çˆ¬å–**
   - å…³é”®è¯æœç´¢
   - è§†é¢‘è¯¦æƒ…è·å–
   - å¹¶å‘æ§åˆ¶

4. **æ•°æ®å¤„ç†**
   - Pydantic æ•°æ®æ¨¡å‹
   - JSON/CSV å­˜å‚¨
   - æ•°æ®åˆ†ææŠ¥å‘Š

5. **å·¥ç¨‹åŒ–è®¾è®¡**
   - æ¨¡å—åŒ–æ¶æ„
   - é…ç½®ç®¡ç†
   - æ—¥å¿—ç³»ç»Ÿ

**å…³é”®è¦ç‚¹ï¼š**

- Bç«™ API éœ€è¦ WBI ç­¾åä¿æŠ¤
- ç™»å½•çŠ¶æ€å¯ä»¥æŒä¹…åŒ–ï¼Œé¿å…é‡å¤æ‰«ç 
- æ³¨æ„æ§åˆ¶çˆ¬å–é¢‘ç‡ï¼Œé¿å…è§¦å‘é™åˆ¶
- åšå¥½å¼‚å¸¸å¤„ç†å’Œæ—¥å¿—è®°å½•
- éµå®ˆ Bç«™ä½¿ç”¨æ¡æ¬¾å’Œæ³•å¾‹æ³•è§„

---

## æ•™ç¨‹æ€»ç»“

æ­å–œä½ å®Œæˆäº† Python çˆ¬è™«è¿›é˜¶æ•™ç¨‹çš„å…¨éƒ¨å†…å®¹ï¼åœ¨è¿™ 11 ç« çš„å­¦ä¹ ä¸­ï¼Œä½ æŒæ¡äº†ï¼š

1. **å·¥ç¨‹åŒ–å¼€å‘**ï¼šæ—¥å¿—ã€é…ç½®ã€å¼‚å¸¸å¤„ç†
2. **åçˆ¬å¯¹æŠ—**ï¼šè¯·æ±‚ä¼ªè£…ã€ä»£ç† IPã€æµè§ˆå™¨æŒ‡çº¹
3. **æµè§ˆå™¨è‡ªåŠ¨åŒ–**ï¼šPlaywright åŸºç¡€å’Œè¿›é˜¶
4. **ç™»å½•è®¤è¯**ï¼šCookie ç®¡ç†ã€æ‰«ç ç™»å½•
5. **éªŒè¯ç å¤„ç†**ï¼šOCR è¯†åˆ«ã€æ»‘å—éªŒè¯
6. **æ•°æ®å¤„ç†**ï¼šæ¸…æ´—ã€å»é‡ã€åˆ†æã€å¯è§†åŒ–

è¿™äº›æŠ€æœ¯éƒ½æºè‡ª [MediaCrawler](https://github.com/NanmiCoder/MediaCrawler) ç­‰å®é™…é¡¹ç›®çš„ç”Ÿäº§å®è·µï¼Œå¸Œæœ›èƒ½å¸®åŠ©ä½ åœ¨çˆ¬è™«å¼€å‘é¢†åŸŸæ›´è¿›ä¸€æ­¥ã€‚

**æœ€åçš„å»ºè®®ï¼š**

- æŒç»­å…³æ³¨åçˆ¬æŠ€æœ¯çš„æ¼”è¿›
- éµå®ˆæ³•å¾‹æ³•è§„å’Œç½‘ç«™è§„åˆ™
- å‚ä¸å¼€æºé¡¹ç›®ï¼Œä¸ç¤¾åŒºå…±åŒæˆé•¿
- å°†çˆ¬è™«ä½œä¸ºæ•°æ®è·å–çš„æ‰‹æ®µï¼Œå…³æ³¨æ•°æ®æœ¬èº«çš„ä»·å€¼

ç¥ä½ åœ¨æ•°æ®é‡‡é›†çš„é“è·¯ä¸Šè¶Šèµ°è¶Šè¿œï¼
