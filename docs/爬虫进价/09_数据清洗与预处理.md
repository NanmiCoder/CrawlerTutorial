# 09_数据清洗与预处理

爬取的原始数据往往包含噪声、冗余和格式不统一等问题。本章将介绍数据清洗的核心技术，帮助你将"脏数据"转换为可用的"干净数据"。

## 一、数据清洗概述

### 1.1 为什么需要数据清洗

爬取的原始数据常见问题：

| 问题类型 | 示例 | 影响 |
|---------|------|------|
| HTML 残留 | `<p>文本</p>` | 数据不纯净 |
| 空白字符 | 多余空格、换行 | 格式混乱 |
| 编码问题 | 乱码、特殊字符 | 无法正常显示 |
| 重复数据 | 相同内容多次出现 | 数据冗余 |
| 格式不统一 | 日期格式各异 | 难以分析 |
| 缺失值 | 空字段 | 数据不完整 |

### 1.2 数据清洗流程

```
原始数据 → 格式清洗 → 内容清洗 → 去重处理 → 标准化 → 干净数据
```

### 1.3 数据质量评估

在清洗前后，评估数据质量：

```python
from dataclasses import dataclass
from typing import List, Dict, Any

@dataclass
class DataQualityReport:
    """数据质量报告"""
    total_records: int
    valid_records: int
    duplicate_count: int
    missing_fields: Dict[str, int]
    format_issues: Dict[str, int]

    @property
    def validity_rate(self) -> float:
        """有效率"""
        return self.valid_records / self.total_records if self.total_records > 0 else 0

    @property
    def duplicate_rate(self) -> float:
        """重复率"""
        return self.duplicate_count / self.total_records if self.total_records > 0 else 0

    def __str__(self) -> str:
        return f"""
数据质量报告:
  总记录数: {self.total_records}
  有效记录: {self.valid_records} ({self.validity_rate:.1%})
  重复记录: {self.duplicate_count} ({self.duplicate_rate:.1%})
  缺失字段: {self.missing_fields}
  格式问题: {self.format_issues}
        """.strip()
```

## 二、文本清洗

### 2.1 HTML 标签移除

```python
import re
from typing import Optional

class HTMLCleaner:
    """HTML 清洗器"""

    # 需要完全移除的标签（包括内容）
    REMOVE_TAGS = ['script', 'style', 'head', 'meta', 'link']

    @staticmethod
    def remove_tags(html: str) -> str:
        """移除所有 HTML 标签"""
        # 先移除特定标签及其内容
        for tag in HTMLCleaner.REMOVE_TAGS:
            pattern = f'<{tag}[^>]*>.*?</{tag}>'
            html = re.sub(pattern, '', html, flags=re.DOTALL | re.IGNORECASE)

        # 移除所有标签
        html = re.sub(r'<[^>]+>', '', html)

        return html

    @staticmethod
    def remove_tags_keep_text(html: str) -> str:
        """移除标签但保留文本内容"""
        # 处理常见的块级元素，添加换行
        html = re.sub(r'</(p|div|br|li|tr|h[1-6])>', '\n', html, flags=re.IGNORECASE)
        # 移除其他标签
        html = re.sub(r'<[^>]+>', '', html)
        return html

    @staticmethod
    def decode_entities(text: str) -> str:
        """解码 HTML 实体"""
        import html
        return html.unescape(text)


# 使用 BeautifulSoup（更可靠）
def clean_html_with_bs4(html: str) -> str:
    """使用 BeautifulSoup 清洗 HTML"""
    try:
        from bs4 import BeautifulSoup
        soup = BeautifulSoup(html, 'html.parser')

        # 移除脚本和样式
        for script in soup(['script', 'style']):
            script.decompose()

        # 获取文本
        text = soup.get_text(separator='\n')
        return text
    except ImportError:
        return HTMLCleaner.remove_tags(html)
```

### 2.2 空白字符处理

```python
class WhitespaceCleaner:
    """空白字符清洗器"""

    @staticmethod
    def normalize_whitespace(text: str) -> str:
        """标准化空白字符"""
        # 将所有空白字符转为普通空格
        text = re.sub(r'[\t\r\f\v]', ' ', text)
        # 合并多个空格
        text = re.sub(r' +', ' ', text)
        # 合并多个换行
        text = re.sub(r'\n+', '\n', text)
        # 去除首尾空白
        return text.strip()

    @staticmethod
    def remove_all_whitespace(text: str) -> str:
        """移除所有空白字符"""
        return re.sub(r'\s+', '', text)

    @staticmethod
    def trim_lines(text: str) -> str:
        """去除每行首尾空白"""
        lines = text.split('\n')
        return '\n'.join(line.strip() for line in lines)

    @staticmethod
    def remove_empty_lines(text: str) -> str:
        """移除空行"""
        lines = text.split('\n')
        return '\n'.join(line for line in lines if line.strip())
```

### 2.3 特殊字符清理

```python
import unicodedata

class SpecialCharCleaner:
    """特殊字符清洗器"""

    @staticmethod
    def remove_control_chars(text: str) -> str:
        """移除控制字符"""
        return ''.join(
            char for char in text
            if unicodedata.category(char) != 'Cc'
        )

    @staticmethod
    def normalize_unicode(text: str, form: str = 'NFKC') -> str:
        """
        Unicode 标准化

        Args:
            text: 输入文本
            form: 标准化形式 (NFC/NFD/NFKC/NFKD)

        Returns:
            标准化后的文本
        """
        return unicodedata.normalize(form, text)

    @staticmethod
    def remove_emojis(text: str) -> str:
        """移除 emoji"""
        emoji_pattern = re.compile(
            "["
            "\U0001F600-\U0001F64F"  # 表情
            "\U0001F300-\U0001F5FF"  # 符号和象形文字
            "\U0001F680-\U0001F6FF"  # 交通和地图
            "\U0001F1E0-\U0001F1FF"  # 旗帜
            "\U00002702-\U000027B0"  # 装饰符号
            "\U000024C2-\U0001F251"  # 封闭字符
            "]+",
            flags=re.UNICODE
        )
        return emoji_pattern.sub('', text)

    @staticmethod
    def to_halfwidth(text: str) -> str:
        """全角转半角"""
        result = []
        for char in text:
            code = ord(char)
            # 全角空格
            if code == 0x3000:
                result.append(' ')
            # 其他全角字符
            elif 0xFF01 <= code <= 0xFF5E:
                result.append(chr(code - 0xFEE0))
            else:
                result.append(char)
        return ''.join(result)
```

### 2.4 编码问题处理

```python
import chardet

class EncodingFixer:
    """编码问题修复器"""

    @staticmethod
    def detect_encoding(data: bytes) -> str:
        """检测编码"""
        result = chardet.detect(data)
        return result['encoding'] or 'utf-8'

    @staticmethod
    def fix_encoding(text: str, source_encoding: str = None) -> str:
        """修复编码问题"""
        try:
            # 如果是乱码，尝试重新编码
            if source_encoding:
                return text.encode('latin1').decode(source_encoding)
            else:
                # 尝试常见编码
                for encoding in ['utf-8', 'gbk', 'gb2312', 'big5']:
                    try:
                        return text.encode('latin1').decode(encoding)
                    except (UnicodeDecodeError, UnicodeEncodeError):
                        continue
        except Exception:
            pass
        return text

    @staticmethod
    def safe_decode(data: bytes, fallback: str = 'utf-8') -> str:
        """安全解码"""
        # 检测编码
        detected = EncodingFixer.detect_encoding(data)
        try:
            return data.decode(detected)
        except (UnicodeDecodeError, TypeError):
            return data.decode(fallback, errors='ignore')
```

## 三、正则表达式高级应用

### 3.1 常用提取模式

```python
class RegexPatterns:
    """常用正则表达式模式"""

    # 基础模式
    CHINESE = r'[\u4e00-\u9fa5]+'  # 中文
    EMAIL = r'[\w.+-]+@[\w-]+\.[\w.-]+'  # 邮箱
    PHONE = r'1[3-9]\d{9}'  # 中国手机号
    URL = r'https?://[^\s<>"{}|\\^`\[\]]+'  # URL
    IP = r'\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}'  # IP 地址

    # 日期时间模式
    DATE_YMD = r'\d{4}[-/年]\d{1,2}[-/月]\d{1,2}日?'  # 年月日
    TIME_HMS = r'\d{1,2}:\d{2}(:\d{2})?'  # 时分秒
    DATETIME = r'\d{4}[-/]\d{1,2}[-/]\d{1,2}\s+\d{1,2}:\d{2}(:\d{2})?'

    # 数值模式
    INTEGER = r'-?\d+'
    FLOAT = r'-?\d+\.?\d*'
    PRICE = r'[¥$￥]\s*\d+\.?\d*'  # 价格
    PERCENTAGE = r'\d+\.?\d*%'  # 百分比


class RegexExtractor:
    """正则表达式提取器"""

    @staticmethod
    def extract_all(text: str, pattern: str) -> list:
        """提取所有匹配"""
        return re.findall(pattern, text)

    @staticmethod
    def extract_first(text: str, pattern: str) -> str:
        """提取第一个匹配"""
        match = re.search(pattern, text)
        return match.group() if match else ''

    @staticmethod
    def extract_groups(text: str, pattern: str) -> dict:
        """提取命名分组"""
        match = re.search(pattern, text)
        return match.groupdict() if match else {}

    @staticmethod
    def extract_between(text: str, start: str, end: str) -> list:
        """提取两个标记之间的内容"""
        pattern = f'{re.escape(start)}(.*?){re.escape(end)}'
        return re.findall(pattern, text, re.DOTALL)
```

### 3.2 高级替换

```python
class RegexReplacer:
    """正则表达式替换器"""

    @staticmethod
    def replace_with_callback(
        text: str,
        pattern: str,
        callback
    ) -> str:
        """使用回调函数替换"""
        return re.sub(pattern, callback, text)

    @staticmethod
    def mask_sensitive(text: str) -> str:
        """脱敏处理"""
        # 手机号脱敏
        text = re.sub(
            r'(1[3-9]\d)\d{4}(\d{4})',
            r'\1****\2',
            text
        )
        # 邮箱脱敏
        text = re.sub(
            r'([\w.+-]{1,3})[\w.+-]*(@[\w-]+\.[\w.-]+)',
            r'\1***\2',
            text
        )
        # 身份证脱敏
        text = re.sub(
            r'(\d{6})\d{8}(\d{4})',
            r'\1********\2',
            text
        )
        return text

    @staticmethod
    def clean_url_params(url: str, keep_params: list = None) -> str:
        """清理 URL 参数"""
        from urllib.parse import urlparse, parse_qs, urlencode, urlunparse

        parsed = urlparse(url)
        params = parse_qs(parsed.query)

        if keep_params:
            params = {k: v for k, v in params.items() if k in keep_params}
        else:
            params = {}

        new_query = urlencode(params, doseq=True)
        return urlunparse(parsed._replace(query=new_query))
```

## 四、数据去重

### 4.1 精确去重

```python
from typing import List, Dict, Any, Callable
import hashlib

class ExactDeduplicator:
    """精确去重器"""

    @staticmethod
    def dedupe_list(items: List[str]) -> List[str]:
        """列表去重（保持顺序）"""
        seen = set()
        result = []
        for item in items:
            if item not in seen:
                seen.add(item)
                result.append(item)
        return result

    @staticmethod
    def dedupe_dicts(
        items: List[Dict],
        key_field: str
    ) -> List[Dict]:
        """字典列表去重"""
        seen = set()
        result = []
        for item in items:
            key = item.get(key_field)
            if key not in seen:
                seen.add(key)
                result.append(item)
        return result

    @staticmethod
    def dedupe_by_hash(
        items: List[Dict],
        fields: List[str]
    ) -> List[Dict]:
        """
        根据多个字段计算哈希去重

        Args:
            items: 数据列表
            fields: 用于计算哈希的字段

        Returns:
            去重后的列表
        """
        seen = set()
        result = []

        for item in items:
            # 计算哈希
            key_str = '|'.join(str(item.get(f, '')) for f in fields)
            key_hash = hashlib.md5(key_str.encode()).hexdigest()

            if key_hash not in seen:
                seen.add(key_hash)
                result.append(item)

        return result
```

### 4.2 模糊去重

```python
class FuzzyDeduplicator:
    """模糊去重器"""

    @staticmethod
    def levenshtein_distance(s1: str, s2: str) -> int:
        """计算编辑距离"""
        if len(s1) < len(s2):
            return FuzzyDeduplicator.levenshtein_distance(s2, s1)

        if len(s2) == 0:
            return len(s1)

        previous_row = range(len(s2) + 1)
        for i, c1 in enumerate(s1):
            current_row = [i + 1]
            for j, c2 in enumerate(s2):
                insertions = previous_row[j + 1] + 1
                deletions = current_row[j] + 1
                substitutions = previous_row[j] + (c1 != c2)
                current_row.append(min(insertions, deletions, substitutions))
            previous_row = current_row

        return previous_row[-1]

    @staticmethod
    def similarity(s1: str, s2: str) -> float:
        """计算相似度 (0-1)"""
        if not s1 or not s2:
            return 0.0
        distance = FuzzyDeduplicator.levenshtein_distance(s1, s2)
        max_len = max(len(s1), len(s2))
        return 1 - distance / max_len

    @staticmethod
    def dedupe_fuzzy(
        items: List[str],
        threshold: float = 0.8
    ) -> List[str]:
        """
        模糊去重

        Args:
            items: 字符串列表
            threshold: 相似度阈值

        Returns:
            去重后的列表
        """
        if not items:
            return []

        result = [items[0]]

        for item in items[1:]:
            is_duplicate = False
            for existing in result:
                if FuzzyDeduplicator.similarity(item, existing) >= threshold:
                    is_duplicate = True
                    break
            if not is_duplicate:
                result.append(item)

        return result
```

## 五、数据标准化

### 5.1 日期时间标准化

```python
from datetime import datetime
from typing import Optional

class DateTimeNormalizer:
    """日期时间标准化器"""

    # 常见日期格式
    DATE_FORMATS = [
        '%Y-%m-%d',
        '%Y/%m/%d',
        '%Y年%m月%d日',
        '%Y.%m.%d',
        '%d-%m-%Y',
        '%m/%d/%Y',
    ]

    # 常见日期时间格式
    DATETIME_FORMATS = [
        '%Y-%m-%d %H:%M:%S',
        '%Y-%m-%d %H:%M',
        '%Y/%m/%d %H:%M:%S',
        '%Y年%m月%d日 %H:%M:%S',
        '%Y年%m月%d日 %H时%M分',
    ]

    @classmethod
    def parse_date(cls, text: str) -> Optional[datetime]:
        """解析日期"""
        text = text.strip()

        for fmt in cls.DATE_FORMATS + cls.DATETIME_FORMATS:
            try:
                return datetime.strptime(text, fmt)
            except ValueError:
                continue

        return None

    @classmethod
    def normalize_date(
        cls,
        text: str,
        output_format: str = '%Y-%m-%d'
    ) -> str:
        """标准化日期格式"""
        dt = cls.parse_date(text)
        if dt:
            return dt.strftime(output_format)
        return text

    @staticmethod
    def parse_relative_time(text: str) -> Optional[datetime]:
        """解析相对时间（如"3小时前"）"""
        import re
        from datetime import timedelta

        now = datetime.now()
        patterns = [
            (r'(\d+)\s*秒前', lambda m: now - timedelta(seconds=int(m.group(1)))),
            (r'(\d+)\s*分钟前', lambda m: now - timedelta(minutes=int(m.group(1)))),
            (r'(\d+)\s*小时前', lambda m: now - timedelta(hours=int(m.group(1)))),
            (r'(\d+)\s*天前', lambda m: now - timedelta(days=int(m.group(1)))),
            (r'刚刚', lambda m: now),
            (r'昨天', lambda m: now - timedelta(days=1)),
            (r'前天', lambda m: now - timedelta(days=2)),
        ]

        for pattern, handler in patterns:
            match = re.search(pattern, text)
            if match:
                return handler(match)

        return None
```

### 5.2 数值标准化

```python
class NumberNormalizer:
    """数值标准化器"""

    @staticmethod
    def parse_number(text: str) -> float:
        """
        解析数字（支持中文单位）

        Examples:
            "1.5万" -> 15000
            "3.2亿" -> 320000000
            "1,234.56" -> 1234.56
        """
        text = text.strip()

        # 中文单位映射
        units = {
            '万': 10000,
            '亿': 100000000,
            'k': 1000,
            'K': 1000,
            'm': 1000000,
            'M': 1000000,
            'b': 1000000000,
            'B': 1000000000,
        }

        multiplier = 1
        for unit, value in units.items():
            if unit in text:
                multiplier = value
                text = text.replace(unit, '')
                break

        # 移除逗号
        text = text.replace(',', '')

        # 提取数字
        match = re.search(r'-?\d+\.?\d*', text)
        if match:
            return float(match.group()) * multiplier

        return 0.0

    @staticmethod
    def format_number(
        value: float,
        precision: int = 2,
        use_units: bool = True
    ) -> str:
        """格式化数字"""
        if not use_units:
            return f'{value:.{precision}f}'

        if value >= 100000000:
            return f'{value/100000000:.{precision}f}亿'
        elif value >= 10000:
            return f'{value/10000:.{precision}f}万'
        else:
            return f'{value:.{precision}f}'
```

### 5.3 文本标准化

```python
class TextNormalizer:
    """文本标准化器"""

    @staticmethod
    def normalize(text: str) -> str:
        """完整的文本标准化流程"""
        # 1. Unicode 标准化
        text = unicodedata.normalize('NFKC', text)
        # 2. 全角转半角
        text = SpecialCharCleaner.to_halfwidth(text)
        # 3. 移除控制字符
        text = SpecialCharCleaner.remove_control_chars(text)
        # 4. 标准化空白
        text = WhitespaceCleaner.normalize_whitespace(text)
        return text

    @staticmethod
    def normalize_punctuation(text: str) -> str:
        """标准化标点符号"""
        # 中文标点转英文
        mapping = {
            '，': ', ',
            '。': '. ',
            '！': '! ',
            '？': '? ',
            '；': '; ',
            '：': ': ',
            '"': '"',
            '"': '"',
            ''': "'",
            ''': "'",
        }
        for cn, en in mapping.items():
            text = text.replace(cn, en)
        return text
```

## 六、综合数据清洗器

```python
from dataclasses import dataclass, field
from typing import List, Dict, Any, Callable

@dataclass
class CleaningConfig:
    """清洗配置"""
    remove_html: bool = True
    normalize_whitespace: bool = True
    normalize_unicode: bool = True
    remove_emojis: bool = False
    to_halfwidth: bool = True

class DataCleaner:
    """综合数据清洗器"""

    def __init__(self, config: CleaningConfig = None):
        self.config = config or CleaningConfig()

    def clean_text(self, text: str) -> str:
        """清洗文本"""
        if not text:
            return ''

        # HTML 清洗
        if self.config.remove_html:
            text = HTMLCleaner.remove_tags(text)
            text = HTMLCleaner.decode_entities(text)

        # Unicode 标准化
        if self.config.normalize_unicode:
            text = SpecialCharCleaner.normalize_unicode(text)

        # 全角转半角
        if self.config.to_halfwidth:
            text = SpecialCharCleaner.to_halfwidth(text)

        # 移除 emoji
        if self.config.remove_emojis:
            text = SpecialCharCleaner.remove_emojis(text)

        # 空白处理
        if self.config.normalize_whitespace:
            text = WhitespaceCleaner.normalize_whitespace(text)

        return text

    def clean_dict(self, data: Dict, text_fields: List[str] = None) -> Dict:
        """清洗字典中的文本字段"""
        result = data.copy()
        text_fields = text_fields or list(data.keys())

        for field in text_fields:
            if field in result and isinstance(result[field], str):
                result[field] = self.clean_text(result[field])

        return result

    def clean_list(
        self,
        items: List[Dict],
        text_fields: List[str] = None
    ) -> List[Dict]:
        """清洗字典列表"""
        return [self.clean_dict(item, text_fields) for item in items]
```

## 七、实战示例

```python
import asyncio

async def demo_data_cleaning():
    """数据清洗演示"""

    # 模拟爬取的原始数据
    raw_data = [
        {
            "title": "  <h1>Python　爬虫教程</h1>  \n\n",
            "content": "<p>这是一篇&amp;关于爬虫的教程。</p>",
            "date": "2024年1月15日",
            "price": "￥99.00",
            "views": "1.5万"
        },
        {
            "title": "<h1>Python　爬虫教程</h1>",  # 重复
            "content": "<p>这是另一篇关于爬虫的教程。</p>",
            "date": "2024-01-15",
            "price": "99元",
            "views": "15000"
        }
    ]

    print("原始数据:")
    for item in raw_data:
        print(f"  {item}")

    # 1. 文本清洗
    cleaner = DataCleaner(CleaningConfig(
        remove_html=True,
        normalize_whitespace=True,
        to_halfwidth=True
    ))

    cleaned_data = cleaner.clean_list(raw_data, ["title", "content"])

    print("\n清洗后数据:")
    for item in cleaned_data:
        print(f"  {item}")

    # 2. 日期标准化
    for item in cleaned_data:
        item["date"] = DateTimeNormalizer.normalize_date(item["date"])

    print("\n日期标准化后:")
    for item in cleaned_data:
        print(f"  date: {item['date']}")

    # 3. 数值标准化
    for item in cleaned_data:
        item["views_num"] = NumberNormalizer.parse_number(item["views"])

    print("\n数值标准化后:")
    for item in cleaned_data:
        print(f"  views: {item['views']} -> {item['views_num']}")

    # 4. 去重
    deduped = ExactDeduplicator.dedupe_by_hash(cleaned_data, ["title"])
    print(f"\n去重后记录数: {len(deduped)}")


if __name__ == "__main__":
    asyncio.run(demo_data_cleaning())
```

## 本章小结

本章介绍了数据清洗与预处理的核心技术：

1. **文本清洗**：HTML 移除、空白处理、特殊字符、编码修复
2. **正则表达式**：常用模式、提取与替换、脱敏处理
3. **数据去重**：精确去重和模糊去重
4. **数据标准化**：日期、数值、文本的统一格式

数据清洗是数据处理流程中的关键环节，干净的数据才能产生有价值的分析结果。

## 下一章预告

下一章我们将学习**数据分析与可视化**，包括使用 pandas 进行数据统计、生成词云、绑制图表等技术。这些技术可以帮助我们从爬取的数据中提取有价值的洞察。
